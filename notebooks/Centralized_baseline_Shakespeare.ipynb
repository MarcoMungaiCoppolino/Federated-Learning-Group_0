{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Download Dataset"
      ],
      "metadata": {
        "id": "BskoLfswjEUx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JojWHJXFP9B-",
        "outputId": "ed19ad66-4d2c-4e9a-c3db-fdaf465332b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'leaf'...\n",
            "remote: Enumerating objects: 782, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 782 (delta 1), reused 2 (delta 0), pack-reused 776\u001b[K\n",
            "Receiving objects: 100% (782/782), 6.79 MiB | 4.91 MiB/s, done.\n",
            "Resolving deltas: 100% (372/372), done.\n",
            "/content/leaf\n",
            "Collecting numpy==1.16.4 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.16.4.zip (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.11.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.13.1 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.13.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/TalwalkarLab/leaf.git\n",
        "%cd leaf\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data/shakespeare\n",
        "!./preprocess.sh --sf 0.2 -t sample -tf 0.8\n",
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUE4MwVQP_Ms",
        "outputId": "5ee0327f-1096-4f05-97ed-1a579997fa2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/leaf/data/shakespeare\n",
            "--2024-06-06 12:44:10--  http://www.gutenberg.org/files/100/old/1994-01-100.zip\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/100/old/1994-01-100.zip [following]\n",
            "--2024-06-06 12:44:11--  https://www.gutenberg.org/files/100/old/1994-01-100.zip\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2035857 (1.9M) [application/zip]\n",
            "Saving to: ‘1994-01-100.zip’\n",
            "\n",
            "1994-01-100.zip     100%[===================>]   1.94M  1.28MB/s    in 1.5s    \n",
            "\n",
            "2024-06-06 12:44:13 (1.28 MB/s) - ‘1994-01-100.zip’ saved [2035857/2035857]\n",
            "\n",
            "Archive:  1994-01-100.zip\n",
            "  inflating: 100.txt                 \n",
            "dividing txt data between users\n",
            "Splitting .txt data between users\n",
            "Discarded 5730 lines\n",
            "generating all_data.json\n",
            "------------------------------\n",
            "generating training and test sets\n",
            "- random seed written out to /content/leaf/data/shakespeare/meta/split_seed.txt\n",
            "splitting data by sample\n",
            "writing all_data_train_9.json\n",
            "writing all_data_test_9.json\n",
            "------------------------------\n",
            "calculating JSON file checksums\n",
            "checksums written to meta/dir-checksum.md5\n",
            "/content/leaf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenize dataset"
      ],
      "metadata": {
        "id": "CLGXZDhUjMR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_LETTERS = \"\\n !\\\"&'(),-.0123456789:;>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz}\"\n",
        "NUM_LETTERS = len(ALL_LETTERS)\n",
        "print(NUM_LETTERS)\n",
        "\n",
        "def _one_hot(index, size):\n",
        "    '''returns one-hot vector with given size and value 1 at given index\n",
        "    '''\n",
        "    vec = [0 for _ in range(size)]\n",
        "    vec[int(index)] = 1\n",
        "    return vec\n",
        "\n",
        "def letter_to_vec(letter):\n",
        "    '''returns one-hot representation of given letter\n",
        "    '''\n",
        "    index = ALL_LETTERS.find(letter)\n",
        "    return _one_hot(index, NUM_LETTERS)\n",
        "\n",
        "def word_to_indices(word):\n",
        "    '''returns a list of character indices\n",
        "\n",
        "    Args:\n",
        "        word: string\n",
        "\n",
        "    Return:\n",
        "        indices: int list with length len(word)\n",
        "    '''\n",
        "    indices = []\n",
        "    for c in word:\n",
        "        indices.append(ALL_LETTERS.find(c))\n",
        "    return indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPTUB5yaQADm",
        "outputId": "e8beb6ee-57d6-43a1-9484-1034cedba11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "def read_data(train_data_path, test_data_path):\n",
        "    def read_file(path):\n",
        "        with open(path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    train_data = read_file(train_data_path)\n",
        "    test_data = read_file(test_data_path)\n",
        "\n",
        "    train_clients = list(train_data['users'])\n",
        "    train_groups = list(train_data['user_data'].keys())\n",
        "    train_data_temp = train_data['user_data']\n",
        "\n",
        "    test_clients = list(test_data['users'])\n",
        "    test_groups = list(test_data['user_data'].keys())\n",
        "    test_data_temp = test_data['user_data']\n",
        "\n",
        "    return train_clients, train_groups, train_data_temp, test_data_temp\n",
        "\n",
        "class ShakeSpeare(Dataset):\n",
        "    def __init__(self, train=True, args=None):\n",
        "        super(ShakeSpeare, self).__init__()\n",
        "        train_clients, train_groups, train_data_temp, test_data_temp = read_data(args.shakespeare_train_path, args.shakespeare_test_path)\n",
        "        self.train = train\n",
        "\n",
        "        if self.train:\n",
        "            self.dic_users = {}\n",
        "            train_data_x = []\n",
        "            train_data_y = []\n",
        "            for i in range(len(train_clients)):\n",
        "                self.dic_users[i] = set()\n",
        "                l = len(train_data_x)\n",
        "                cur_x = train_data_temp[train_clients[i]]['x']\n",
        "                cur_y = train_data_temp[train_clients[i]]['y']\n",
        "                for j in range(len(cur_x)):\n",
        "                    self.dic_users[i].add(j + l)\n",
        "                    train_data_x.append(cur_x[j])\n",
        "                    train_data_y.append(cur_y[j])\n",
        "            self.data = train_data_x\n",
        "            self.label = train_data_y\n",
        "        else:\n",
        "            test_data_x = []\n",
        "            test_data_y = []\n",
        "            for i in range(len(train_clients)):\n",
        "                cur_x = test_data_temp[train_clients[i]]['x']\n",
        "                cur_y = test_data_temp[train_clients[i]]['y']\n",
        "                for j in range(len(cur_x)):\n",
        "                    test_data_x.append(cur_x[j])\n",
        "                    test_data_y.append(cur_y[j])\n",
        "            self.data = test_data_x\n",
        "            self.label = test_data_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence, target = self.data[index], self.label[index]\n",
        "        indices = word_to_indices(sentence)\n",
        "        target = letter_to_vec(target)\n",
        "        indices = torch.LongTensor(np.array(indices))\n",
        "        target = torch.FloatTensor(np.array(target))\n",
        "        return indices, target\n",
        "\n",
        "    def get_client_dic(self):\n",
        "        if self.train:\n",
        "            return self.dic_users\n",
        "        else:\n",
        "            raise ValueError(\"The test dataset does not have dic_users!\")"
      ],
      "metadata": {
        "id": "EJG_2n6AQJPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "KuXF199bjSpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CharLSTM, self).__init__()\n",
        "\n",
        "        embedding_dim = 8\n",
        "        hidden_size = 100\n",
        "        num_LSTM = 2\n",
        "        input_length = 80\n",
        "        self.n_cls = 80\n",
        "\n",
        "        self.embedding = nn.Embedding(input_length, embedding_dim)\n",
        "        self.stacked_LSTM = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_LSTM)\n",
        "        self.fc = nn.Linear(hidden_size, self.n_cls)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(1, 0, 2) # lstm accepts in this style\n",
        "        output, (h_, c_) = self.stacked_LSTM(x)\n",
        "        # Choose last hidden layer\n",
        "        last_hidden = output[-1,:,:]\n",
        "        x = self.fc(last_hidden)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "wkVqV6J-Mwfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training parameter"
      ],
      "metadata": {
        "id": "-Agntu9xjbiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "model = CharLSTM()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Ciclo di addestramento\n",
        "epochs = 10\n",
        "\n",
        "# Define the optimizer (Stochastic Gradient Descent with Momentum)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "zSmqWieNQjXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    shakespeare_train_path = '/content/leaf/data/shakespeare/data/train/all_data_train_9.json'\n",
        "    shakespeare_test_path = '/content/leaf/data/shakespeare/data/test/all_data_test_9.json'\n",
        "\n",
        "args = Args()\n",
        "train_dataset = ShakeSpeare(train=True, args=args)\n",
        "test_dataset = ShakeSpeare(train=False, args=args)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "jnQSQ2MdQo3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Plot"
      ],
      "metadata": {
        "id": "96crVa22D4vZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Definition test and test"
      ],
      "metadata": {
        "id": "uDnYbl-nq4Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 99:  # Print every 100 batches\n",
        "            #print(f'Batch {batch_idx + 1}, Loss: {running_loss / 100:.6f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "def test(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output, target)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Get the index of the max log-probability\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            _, target_idx = torch.max(target, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target_idx).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Test Loss: {test_loss:.6f}, Accuracy: {correct}/{total} ({accuracy:.2f}%)')\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzkMfP8yQ635",
        "outputId": "71e232aa-0da4-4f5b-ddc4-ba0d9de2bcc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharLSTM(\n",
              "  (embedding): Embedding(80, 8)\n",
              "  (stacked_LSTM): LSTM(8, 100, num_layers=2)\n",
              "  (fc): Linear(in_features=100, out_features=80, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and plot"
      ],
      "metadata": {
        "id": "EEqzbKE2rDQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize lists to store test loss and accuracy\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Training settings\n",
        "epochs = 25 #20\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f'Epoch {epoch}/{epochs}')\n",
        "    train(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, accuracy = test(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(accuracy)\n",
        "\n",
        "# Plot test loss\n",
        "plt.plot(range(1, epochs + 1), test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Test Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig('test_loss_plot.pdf')\n",
        "plt.close()\n",
        "\n",
        "# Plot test accuracy\n",
        "plt.plot(range(1, epochs + 1), test_accuracies, label='Test Accuracy', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Test Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig('test_accuracy_plot.pdf')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "1vyLklZNqkMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34020e92-e6f2-4784-c0e7-b3afcec2887c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "Test Loss: 0.026906, Accuracy: 176968/356921 (49.58%)\n",
            "Epoch 2/25\n",
            "Test Loss: 0.025335, Accuracy: 185714/356921 (52.03%)\n",
            "Epoch 3/25\n",
            "Test Loss: 0.024805, Accuracy: 188939/356921 (52.94%)\n",
            "Epoch 4/25\n",
            "Test Loss: 0.024265, Accuracy: 191702/356921 (53.71%)\n",
            "Epoch 5/25\n",
            "Test Loss: 0.023991, Accuracy: 192863/356921 (54.04%)\n",
            "Epoch 6/25\n",
            "Test Loss: 0.023848, Accuracy: 194146/356921 (54.39%)\n",
            "Epoch 7/25\n",
            "Test Loss: 0.023724, Accuracy: 194107/356921 (54.38%)\n",
            "Epoch 8/25\n",
            "Test Loss: 0.023688, Accuracy: 194589/356921 (54.52%)\n",
            "Epoch 9/25\n",
            "Test Loss: 0.023562, Accuracy: 195374/356921 (54.74%)\n",
            "Epoch 10/25\n",
            "Test Loss: 0.023396, Accuracy: 196232/356921 (54.98%)\n",
            "Epoch 11/25\n",
            "Test Loss: 0.023354, Accuracy: 196392/356921 (55.02%)\n",
            "Epoch 12/25\n",
            "Test Loss: 0.023371, Accuracy: 196197/356921 (54.97%)\n",
            "Epoch 13/25\n",
            "Test Loss: 0.023314, Accuracy: 196950/356921 (55.18%)\n",
            "Epoch 14/25\n",
            "Test Loss: 0.023330, Accuracy: 197011/356921 (55.20%)\n",
            "Epoch 15/25\n",
            "Test Loss: 0.023295, Accuracy: 196451/356921 (55.04%)\n",
            "Epoch 16/25\n",
            "Test Loss: 0.023242, Accuracy: 197063/356921 (55.21%)\n",
            "Epoch 17/25\n",
            "Test Loss: 0.023184, Accuracy: 197706/356921 (55.39%)\n",
            "Epoch 18/25\n",
            "Test Loss: 0.023191, Accuracy: 197399/356921 (55.31%)\n",
            "Epoch 19/25\n",
            "Test Loss: 0.023030, Accuracy: 198689/356921 (55.67%)\n",
            "Epoch 20/25\n",
            "Test Loss: 0.023063, Accuracy: 198137/356921 (55.51%)\n",
            "Epoch 21/25\n",
            "Test Loss: 0.023031, Accuracy: 198560/356921 (55.63%)\n",
            "Epoch 22/25\n",
            "Test Loss: 0.023016, Accuracy: 198464/356921 (55.60%)\n",
            "Epoch 23/25\n",
            "Test Loss: 0.023103, Accuracy: 197634/356921 (55.37%)\n",
            "Epoch 24/25\n",
            "Test Loss: 0.023088, Accuracy: 198171/356921 (55.52%)\n",
            "Epoch 25/25\n",
            "Test Loss: 0.022993, Accuracy: 198631/356921 (55.65%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "86cckydHDsZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lr e wd"
      ],
      "metadata": {
        "id": "9EOqFu-plL-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "# Define a function for model training and testing\n",
        "def train_and_test(model, train_loader, test_loader, criterion, optimizer, device):\n",
        "    model.to(device)\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f'Epoch {epoch}/{epochs}')\n",
        "        train(model, train_loader, criterion, optimizer, device)\n",
        "        test_loss, accuracy = test(model, test_loader, criterion, device)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(accuracy)\n",
        "\n",
        "    return test_losses, test_accuracies\n",
        "\n",
        "# Define hyperparameters and ranges for LR and WD\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "weight_decays = [0.0001, 0.001, 0.01]\n",
        "\n",
        "best_lr = None\n",
        "best_wd = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Iterate over all combinations of LR and WD\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        # Define your model, optimizer, and criterion\n",
        "        model = CharLSTM()  # Replace YourModel with your actual model class\n",
        "        optimizer = optim.SGD(model.parameters(), momentum=0.9, lr=lr, weight_decay=wd)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Train and test the model\n",
        "        test_losses, test_accuracies = train_and_test(model, train_loader, test_loader, criterion, optimizer, device)\n",
        "\n",
        "        # Get the final accuracy\n",
        "        final_accuracy = test_accuracies[-1]\n",
        "\n",
        "        # Check if this combination gives the best accuracy so far\n",
        "        if final_accuracy > best_accuracy:\n",
        "            best_lr = lr\n",
        "            best_wd = wd\n",
        "            best_accuracy = final_accuracy\n",
        "\n",
        "# Print the best LR, WD, and accuracy\n",
        "print(f'Best LR: {best_lr}, Best WD: {best_wd}, Best Accuracy: {best_accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "Qc5m7n6n-YCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f46c497-8f7a-4f24-abc0-a7f3922226d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Test Loss: 0.039672, Accuracy: 109243/356921 (30.61%)\n",
            "Epoch 2/10\n",
            "Test Loss: 0.034310, Accuracy: 138947/356921 (38.93%)\n",
            "Epoch 3/10\n",
            "Test Loss: 0.031625, Accuracy: 152499/356921 (42.73%)\n",
            "Epoch 4/10\n",
            "Test Loss: 0.029899, Accuracy: 162285/356921 (45.47%)\n",
            "Epoch 5/10\n",
            "Test Loss: 0.028831, Accuracy: 168456/356921 (47.20%)\n",
            "Epoch 6/10\n",
            "Test Loss: 0.028122, Accuracy: 171772/356921 (48.13%)\n",
            "Epoch 7/10\n",
            "Test Loss: 0.027478, Accuracy: 174779/356921 (48.97%)\n",
            "Epoch 8/10\n",
            "Test Loss: 0.027100, Accuracy: 176878/356921 (49.56%)\n",
            "Epoch 9/10\n",
            "Test Loss: 0.026718, Accuracy: 178798/356921 (50.09%)\n",
            "Epoch 10/10\n",
            "Test Loss: 0.026420, Accuracy: 180007/356921 (50.43%)\n",
            "Epoch 1/10\n",
            "Test Loss: 0.045084, Accuracy: 87337/356921 (24.47%)\n",
            "Epoch 2/10\n",
            "Test Loss: 0.038091, Accuracy: 116294/356921 (32.58%)\n",
            "Epoch 3/10\n",
            "Test Loss: 0.036379, Accuracy: 131086/356921 (36.73%)\n",
            "Epoch 4/10\n",
            "Test Loss: 0.035041, Accuracy: 135969/356921 (38.09%)\n",
            "Epoch 5/10\n",
            "Test Loss: 0.034169, Accuracy: 140854/356921 (39.46%)\n",
            "Epoch 6/10\n",
            "Test Loss: 0.033379, Accuracy: 144931/356921 (40.61%)\n",
            "Epoch 7/10\n",
            "Test Loss: 0.032809, Accuracy: 147208/356921 (41.24%)\n",
            "Epoch 8/10\n",
            "Test Loss: 0.032279, Accuracy: 149930/356921 (42.01%)\n",
            "Epoch 9/10\n",
            "Test Loss: 0.032038, Accuracy: 150647/356921 (42.21%)\n",
            "Epoch 10/10\n",
            "Test Loss: 0.031763, Accuracy: 152387/356921 (42.69%)\n",
            "Epoch 1/10\n",
            "Test Loss: 0.049147, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 2/10\n",
            "Test Loss: 0.049151, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 3/10\n",
            "Test Loss: 0.049151, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 4/10\n",
            "Test Loss: 0.049175, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 5/10\n",
            "Test Loss: 0.049140, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 6/10\n",
            "Test Loss: 0.049158, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 7/10\n",
            "Test Loss: 0.049146, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 8/10\n",
            "Test Loss: 0.049142, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 9/10\n",
            "Test Loss: 0.049149, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 10/10\n",
            "Test Loss: 0.049141, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 1/10\n",
            "Test Loss: 0.027210, Accuracy: 175638/356921 (49.21%)\n",
            "Epoch 2/10\n",
            "Test Loss: 0.025290, Accuracy: 186388/356921 (52.22%)\n",
            "Epoch 3/10\n",
            "Test Loss: 0.024669, Accuracy: 190039/356921 (53.24%)\n",
            "Epoch 4/10\n",
            "Test Loss: 0.024289, Accuracy: 191952/356921 (53.78%)\n",
            "Epoch 5/10\n",
            "Test Loss: 0.023952, Accuracy: 193702/356921 (54.27%)\n",
            "Epoch 6/10\n",
            "Test Loss: 0.023856, Accuracy: 194390/356921 (54.46%)\n",
            "Epoch 7/10\n",
            "Test Loss: 0.023737, Accuracy: 194635/356921 (54.53%)\n",
            "Epoch 8/10\n",
            "Test Loss: 0.023598, Accuracy: 195244/356921 (54.70%)\n",
            "Epoch 9/10\n",
            "Test Loss: 0.023509, Accuracy: 195627/356921 (54.81%)\n",
            "Epoch 10/10\n",
            "Test Loss: 0.023419, Accuracy: 196167/356921 (54.96%)\n",
            "Epoch 1/10\n",
            "Test Loss: 0.032619, Accuracy: 146885/356921 (41.15%)\n",
            "Epoch 2/10\n",
            "Test Loss: 0.031058, Accuracy: 154634/356921 (43.32%)\n",
            "Epoch 3/10\n",
            "Test Loss: 0.030645, Accuracy: 157464/356921 (44.12%)\n",
            "Epoch 4/10\n",
            "Test Loss: 0.030397, Accuracy: 157234/356921 (44.05%)\n",
            "Epoch 5/10\n",
            "Test Loss: 0.030250, Accuracy: 158611/356921 (44.44%)\n",
            "Epoch 6/10\n",
            "Test Loss: 0.030057, Accuracy: 159969/356921 (44.82%)\n",
            "Epoch 7/10\n",
            "Test Loss: 0.030043, Accuracy: 160617/356921 (45.00%)\n",
            "Epoch 8/10\n",
            "Test Loss: 0.029969, Accuracy: 160658/356921 (45.01%)\n",
            "Epoch 9/10\n",
            "Test Loss: 0.029987, Accuracy: 161274/356921 (45.18%)\n",
            "Epoch 10/10\n",
            "Test Loss: 0.029869, Accuracy: 162087/356921 (45.41%)\n",
            "Epoch 1/10\n",
            "Test Loss: 0.049794, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 2/10\n",
            "Test Loss: 0.049386, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 3/10\n",
            "Test Loss: 0.049371, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 4/10\n",
            "Test Loss: 0.049312, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 5/10\n",
            "Test Loss: 0.049527, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 6/10\n",
            "Test Loss: 0.049319, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 7/10\n",
            "Test Loss: 0.049582, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 8/10\n",
            "Test Loss: 0.049360, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 9/10\n",
            "Test Loss: 0.049438, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 10/10\n",
            "Test Loss: 0.049275, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 1/10\n",
            "Test Loss: 0.025567, Accuracy: 182157/356921 (51.04%)\n",
            "Epoch 2/10\n",
            "Test Loss: 0.025109, Accuracy: 184807/356921 (51.78%)\n",
            "Epoch 3/10\n",
            "Test Loss: 0.024903, Accuracy: 186302/356921 (52.20%)\n",
            "Epoch 4/10\n",
            "Test Loss: 0.024906, Accuracy: 187090/356921 (52.42%)\n",
            "Epoch 5/10\n",
            "Test Loss: 0.024937, Accuracy: 185785/356921 (52.05%)\n",
            "Epoch 6/10\n",
            "Test Loss: 0.024740, Accuracy: 187341/356921 (52.49%)\n",
            "Epoch 7/10\n",
            "Test Loss: 0.024699, Accuracy: 187838/356921 (52.63%)\n",
            "Epoch 8/10\n",
            "Test Loss: 0.024834, Accuracy: 186909/356921 (52.37%)\n",
            "Epoch 9/10\n",
            "Test Loss: 0.024639, Accuracy: 187706/356921 (52.59%)\n",
            "Epoch 10/10\n",
            "Test Loss: 0.024515, Accuracy: 188737/356921 (52.88%)\n",
            "Epoch 1/10\n",
            "Test Loss: 0.033031, Accuracy: 143181/356921 (40.12%)\n",
            "Epoch 2/10\n",
            "Test Loss: 0.033207, Accuracy: 142511/356921 (39.93%)\n",
            "Epoch 3/10\n",
            "Test Loss: 0.032721, Accuracy: 146063/356921 (40.92%)\n",
            "Epoch 4/10\n",
            "Test Loss: 0.032604, Accuracy: 146783/356921 (41.12%)\n",
            "Epoch 5/10\n",
            "Test Loss: 0.032722, Accuracy: 144765/356921 (40.56%)\n",
            "Epoch 6/10\n",
            "Test Loss: 0.033060, Accuracy: 143068/356921 (40.08%)\n",
            "Epoch 7/10\n",
            "Test Loss: 0.032528, Accuracy: 143805/356921 (40.29%)\n",
            "Epoch 8/10\n",
            "Test Loss: 0.033588, Accuracy: 139509/356921 (39.09%)\n",
            "Epoch 9/10\n",
            "Test Loss: 0.032793, Accuracy: 144313/356921 (40.43%)\n",
            "Epoch 10/10\n",
            "Test Loss: 0.032800, Accuracy: 145331/356921 (40.72%)\n",
            "Epoch 1/10\n",
            "Test Loss: 0.050000, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 2/10\n",
            "Test Loss: 0.050384, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 3/10\n",
            "Test Loss: 0.050156, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 4/10\n",
            "Test Loss: 0.050780, Accuracy: 22136/356921 (6.20%)\n",
            "Epoch 5/10\n",
            "Test Loss: 0.050388, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 6/10\n",
            "Test Loss: 0.050074, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 7/10\n",
            "Test Loss: 0.050006, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 8/10\n",
            "Test Loss: 0.050068, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 9/10\n",
            "Test Loss: 0.050079, Accuracy: 67635/356921 (18.95%)\n",
            "Epoch 10/10\n",
            "Test Loss: 0.050059, Accuracy: 67635/356921 (18.95%)\n",
            "Best LR: 0.01, Best WD: 0.0001, Best Accuracy: 54.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##With Scheduler"
      ],
      "metadata": {
        "id": "24APKX2CDyKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Initialize lists to store test loss and accuracy\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Training settings\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f'Epoch {epoch}/{epochs}')\n",
        "    train(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, accuracy = test(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(accuracy)\n",
        "\n",
        "# Plot test loss\n",
        "plt.plot(range(1, epochs + 1), test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Test Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig('test_loss_plot.pdf')\n",
        "plt.close()\n",
        "\n",
        "# Plot test accuracy\n",
        "plt.plot(range(1, epochs + 1), test_accuracies, label='Test Accuracy', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Test Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig('test_accuracy_plot.pdf')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "Og3F7DFTaUhB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2477d41-2e17-4044-e068-ab59f6f27fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Test Loss: 0.025248, Accuracy: 186061/356921 (52.13%)\n",
            "Epoch 2/20\n",
            "Test Loss: 0.024494, Accuracy: 190147/356921 (53.27%)\n",
            "Epoch 3/20\n",
            "Test Loss: 0.024182, Accuracy: 191807/356921 (53.74%)\n",
            "Epoch 4/20\n",
            "Test Loss: 0.023956, Accuracy: 192783/356921 (54.01%)\n",
            "Epoch 5/20\n",
            "Test Loss: 0.023752, Accuracy: 194155/356921 (54.40%)\n",
            "Epoch 6/20\n",
            "Test Loss: 0.023493, Accuracy: 195485/356921 (54.77%)\n",
            "Epoch 7/20\n",
            "Test Loss: 0.023405, Accuracy: 196465/356921 (55.04%)\n",
            "Epoch 8/20\n",
            "Test Loss: 0.023399, Accuracy: 196490/356921 (55.05%)\n",
            "Epoch 9/20\n",
            "Test Loss: 0.023310, Accuracy: 197214/356921 (55.25%)\n",
            "Epoch 10/20\n",
            "Test Loss: 0.023255, Accuracy: 197531/356921 (55.34%)\n",
            "Epoch 11/20\n",
            "Test Loss: 0.023247, Accuracy: 197299/356921 (55.28%)\n",
            "Epoch 12/20\n",
            "Test Loss: 0.023208, Accuracy: 198107/356921 (55.50%)\n",
            "Epoch 13/20\n",
            "Test Loss: 0.023182, Accuracy: 197617/356921 (55.37%)\n",
            "Epoch 14/20\n",
            "Test Loss: 0.023204, Accuracy: 197403/356921 (55.31%)\n",
            "Epoch 15/20\n",
            "Test Loss: 0.023167, Accuracy: 197101/356921 (55.22%)\n",
            "Epoch 16/20\n",
            "Test Loss: 0.023048, Accuracy: 198685/356921 (55.67%)\n",
            "Epoch 17/20\n",
            "Test Loss: 0.023042, Accuracy: 198191/356921 (55.53%)\n",
            "Epoch 18/20\n",
            "Test Loss: 0.023003, Accuracy: 199043/356921 (55.77%)\n",
            "Epoch 19/20\n",
            "Test Loss: 0.022983, Accuracy: 198624/356921 (55.65%)\n",
            "Epoch 20/20\n",
            "Test Loss: 0.022992, Accuracy: 198796/356921 (55.70%)\n"
          ]
        }
      ]
    }
  ]
}