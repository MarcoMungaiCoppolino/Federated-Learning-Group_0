{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# New Hypernetwork & Model"
      ],
      "metadata": {
        "id": "pTnZLRh_U1me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset, SubsetRandomSampler"
      ],
      "metadata": {
        "id": "Cat6UBgs0IUI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "class CNNHyper(nn.Module):\n",
        "    def __init__(\n",
        "            self, n_nodes, embedding_dim, in_channels=3, out_dim=100, n_kernels=64, hidden_dim=100,\n",
        "            spec_norm=False, n_hidden=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_dim = out_dim\n",
        "        self.n_kernels = n_kernels\n",
        "        self.embeddings = nn.Embedding(num_embeddings=n_nodes, embedding_dim=embedding_dim)\n",
        "\n",
        "        layers = [\n",
        "            spectral_norm(nn.Linear(embedding_dim, hidden_dim)) if spec_norm else nn.Linear(embedding_dim, hidden_dim),\n",
        "        ]\n",
        "        for _ in range(n_hidden):\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            layers.append(\n",
        "                spectral_norm(nn.Linear(hidden_dim, hidden_dim)) if spec_norm else nn.Linear(hidden_dim, hidden_dim),\n",
        "            )\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "        self.c1_weights = nn.Linear(hidden_dim, self.n_kernels * self.in_channels * 5 * 5)\n",
        "        self.c1_bias = nn.Linear(hidden_dim, self.n_kernels)\n",
        "        self.c2_weights = nn.Linear(hidden_dim, self.n_kernels * self.n_kernels * 5 * 5)\n",
        "        self.c2_bias = nn.Linear(hidden_dim, self.n_kernels)\n",
        "        self.l1_weights = nn.Linear(hidden_dim, 384 * 64 * 5 * 5)\n",
        "        self.l1_bias = nn.Linear(hidden_dim, 384)\n",
        "        self.l2_weights = nn.Linear(hidden_dim, 192 * 384)\n",
        "        self.l2_bias = nn.Linear(hidden_dim, 192)\n",
        "        self.l3_weights = nn.Linear(hidden_dim, self.out_dim * 192)\n",
        "        self.l3_bias = nn.Linear(hidden_dim, self.out_dim)\n",
        "\n",
        "        if spec_norm:\n",
        "            self.c1_weights = spectral_norm(self.c1_weights)\n",
        "            self.c1_bias = spectral_norm(self.c1_bias)\n",
        "            self.c2_weights = spectral_norm(self.c2_weights)\n",
        "            self.c2_bias = spectral_norm(self.c2_bias)\n",
        "            self.l1_weights = spectral_norm(self.l1_weights)\n",
        "            self.l1_bias = spectral_norm(self.l1_bias)\n",
        "            self.l2_weights = spectral_norm(self.l2_weights)\n",
        "            self.l2_bias = spectral_norm(self.l2_bias)\n",
        "            self.l3_weights = spectral_norm(self.l3_weights)\n",
        "            self.l3_bias = spectral_norm(self.l3_bias)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        emd = self.embeddings(idx)\n",
        "        features = self.mlp(emd)\n",
        "\n",
        "        weights = OrderedDict({\n",
        "            \"conv1.weight\": self.c1_weights(features).view(self.n_kernels, self.in_channels, 5, 5),\n",
        "            \"conv1.bias\": self.c1_bias(features).view(-1),\n",
        "            \"conv2.weight\": self.c2_weights(features).view(self.n_kernels, self.n_kernels, 5, 5),\n",
        "            \"conv2.bias\": self.c2_bias(features).view(-1),\n",
        "            \"fc1.weight\": self.l1_weights(features).view(384, self.n_kernels * 5 * 5),\n",
        "            \"fc1.bias\": self.l1_bias(features).view(-1),\n",
        "            \"fc2.weight\": self.l2_weights(features).view(192, 384),\n",
        "            \"fc2.bias\": self.l2_bias(features).view(-1),\n",
        "            \"fc3.weight\": self.l3_weights(features).view(self.out_dim, 192),\n",
        "            \"fc3.bias\": self.l3_bias(features).view(-1),\n",
        "        })\n",
        "        return weights\n",
        "\n",
        "\n",
        "class CIFARLeNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network model inspired by LeNet5, designed for CIFAR-100 dataset.\n",
        "\n",
        "    Attributes:\n",
        "    ----------\n",
        "    flatten : nn.Module\n",
        "        A layer to flatten the input tensor.\n",
        "    conv1 : nn.Module\n",
        "        First convolutional layer with 3 input channels and 64 output channels.\n",
        "    conv2 : nn.Module\n",
        "        Second convolutional layer with 64 input channels and 64 output channels.\n",
        "    pool : nn.Module\n",
        "        Max pooling layer with kernel size of 2.\n",
        "    fc1 : nn.Module\n",
        "        Fully connected layer with input size 64*5*5 and output size 384.\n",
        "    fc2 : nn.Module\n",
        "        Fully connected layer with input size 384 and output size 192.\n",
        "    fc3 : nn.Module\n",
        "        Fully connected layer with input size 192 and output size 100.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the CIFARLeNet model with its layers.\n",
        "        \"\"\"\n",
        "        super(CIFARLeNet, self).__init__()\n",
        "        # self.flatten = nn.Flatten()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "    def produce_feature(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            The input tensor.\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The output tensor after applying all the layers.\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        #x = x.view(-1, 64 * 5 * 5)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x.shape[0], -1\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        # x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nmKL3oQQU0dv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "class CNNHyper_old(nn.Module):\n",
        "    def __init__(\n",
        "            self, n_nodes, embedding_dim, in_channels=3, out_dim=100, n_kernels=64, hidden_dim=100,\n",
        "            spec_norm=False, n_hidden=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_dim = out_dim\n",
        "        self.n_kernels = n_kernels\n",
        "        self.embeddings = nn.Embedding(num_embeddings=n_nodes, embedding_dim=embedding_dim)\n",
        "\n",
        "        layers = [\n",
        "            spectral_norm(nn.Linear(embedding_dim, hidden_dim)) if spec_norm else nn.Linear(embedding_dim, hidden_dim),\n",
        "        ]\n",
        "        for _ in range(n_hidden):\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            layers.append(\n",
        "                spectral_norm(nn.Linear(hidden_dim, hidden_dim)) if spec_norm else nn.Linear(hidden_dim, hidden_dim),\n",
        "            )\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "        self.c1_weights = nn.Linear(hidden_dim, self.n_kernels * self.in_channels * 5 * 5)\n",
        "        self.c1_bias = nn.Linear(hidden_dim, self.n_kernels)\n",
        "        self.c2_weights = nn.Linear(hidden_dim, self.n_kernels * self.n_kernels * 5 * 5)\n",
        "        self.c2_bias = nn.Linear(hidden_dim, self.n_kernels)\n",
        "        self.l1_weights = nn.Linear(hidden_dim, 384 * 64 * 5 * 5)\n",
        "        self.l1_bias = nn.Linear(hidden_dim, 384)\n",
        "        self.l2_weights = nn.Linear(hidden_dim, 192 * 384)\n",
        "        self.l2_bias = nn.Linear(hidden_dim, 192)\n",
        "        self.l3_weights = nn.Linear(hidden_dim, self.out_dim * 192)\n",
        "        self.l3_bias = nn.Linear(hidden_dim, self.out_dim)\n",
        "\n",
        "        if spec_norm:\n",
        "            self.c1_weights = spectral_norm(self.c1_weights)\n",
        "            self.c1_bias = spectral_norm(self.c1_bias)\n",
        "            self.c2_weights = spectral_norm(self.c2_weights)\n",
        "            self.c2_bias = spectral_norm(self.c2_bias)\n",
        "            self.l1_weights = spectral_norm(self.l1_weights)\n",
        "            self.l1_bias = spectral_norm(self.l1_bias)\n",
        "            self.l2_weights = spectral_norm(self.l2_weights)\n",
        "            self.l2_bias = spectral_norm(self.l2_bias)\n",
        "            self.l3_weights = spectral_norm(self.l3_weights)\n",
        "            self.l3_bias = spectral_norm(self.l3_bias)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        emd = self.embeddings(idx)\n",
        "        features = self.mlp(emd)\n",
        "\n",
        "        weights = OrderedDict({\n",
        "            \"conv1.weight\": self.c1_weights(features).view(self.n_kernels, self.in_channels, 5, 5),\n",
        "            \"conv1.bias\": self.c1_bias(features).view(-1),\n",
        "            \"conv2.weight\": self.c2_weights(features).view(self.n_kernels, self.n_kernels, 5, 5),\n",
        "            \"conv2.bias\": self.c2_bias(features).view(-1),\n",
        "            \"fc1.weight\": self.l1_weights(features).view(384, self.n_kernels * 5 * 5),\n",
        "            \"fc1.bias\": self.l1_bias(features).view(-1),\n",
        "            \"fc2.weight\": self.l2_weights(features).view(192, 384),\n",
        "            \"fc2.bias\": self.l2_bias(features).view(-1),\n",
        "            \"fc3.weight\": self.l3_weights(features).view(self.out_dim, 192),\n",
        "            \"fc3.bias\": self.l3_bias(features).view(-1),\n",
        "        })\n",
        "        return weights\n",
        "\n",
        "\n",
        "class CIFARLeNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network model inspired by LeNet5, designed for CIFAR-100 dataset.\n",
        "\n",
        "    Attributes:\n",
        "    ----------\n",
        "    flatten : nn.Module\n",
        "        A layer to flatten the input tensor.\n",
        "    conv1 : nn.Module\n",
        "        First convolutional layer with 3 input channels and 64 output channels.\n",
        "    conv2 : nn.Module\n",
        "        Second convolutional layer with 64 input channels and 64 output channels.\n",
        "    pool : nn.Module\n",
        "        Max pooling layer with kernel size of 2.\n",
        "    fc1 : nn.Module\n",
        "        Fully connected layer with input size 64*5*5 and output size 384.\n",
        "    fc2 : nn.Module\n",
        "        Fully connected layer with input size 384 and output size 192.\n",
        "    fc3 : nn.Module\n",
        "        Fully connected layer with input size 192 and output size 100.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the CIFARLeNet model with its layers.\n",
        "        \"\"\"\n",
        "        super(CIFARLeNet, self).__init__()\n",
        "        # self.flatten = nn.Flatten()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "    def produce_feature(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            The input tensor.\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The output tensor after applying all the layers.\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        #x = x.view(-1, 64 * 5 * 5)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x.shape[0], -1\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        # x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AeAISCEECK4T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Args Parser\n"
      ],
      "metadata": {
        "id": "597DBC80SM2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "\n",
        "def str2bool(v): # this is an utils function\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "def args_parser():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    ## Wandb parameters\n",
        "    parser.add_argument('--wandb_key', type=str, default='', help='wandb key')\n",
        "    parser.add_argument('--wandb_username', type=str, default='', help='wandb userna,,e')\n",
        "    parser.add_argument('--wandb_project', type=str, default='charBert', help='wandb project')\n",
        "    parser.add_argument('--wandb_run_name', type=str, default='charBert', help='wandb run name')\n",
        "    parser.add_argument('--logfile', type=str, default='/content/logger.log', help='log file name')\n",
        "    parser.add_argument('--data_dir', type=str, default='./data', help='data directory')\n",
        "\n",
        "    # federated arguments (Notation for the arguments followed from paper)\n",
        "    parser.add_argument('--epochs', type=int, default=2000,\n",
        "                        help=\"number of rounds of training\")\n",
        "    parser.add_argument('--last_epoch', type=int, default=1900,\n",
        "                        help=\"number of rounds of old clients training\")\n",
        "    parser.add_argument('--num_users', type=int, default=100,\n",
        "                        help=\"number of total users: K\")\n",
        "    parser.add_argument('--n_nodes', type=int, default=90,\n",
        "                        help=\"number of already seen users\")\n",
        "    parser.add_argument('--Nc', type=int, default=5,\n",
        "                        help='number of class each client in non iid')\n",
        "    parser.add_argument('--frac', type=float, default=0.1,\n",
        "                        help='the fraction of clients: C')\n",
        "    parser.add_argument('--local_ep', type=int, default=4,\n",
        "                        help=\"the number of local rounds: J\")\n",
        "    parser.add_argument('--local_bs', type=int, default=64,\n",
        "                        help=\"local batch size: B\")\n",
        "    parser.add_argument('--lr', type=float, default=0.01,\n",
        "                        help='learning rate')\n",
        "    parser.add_argument('--inner_lr', type=float, default=0.01,\n",
        "                        help='local learning rate')\n",
        "    parser.add_argument(\"--wd\", type=float, default=4e-4,\n",
        "                        help=\"weight decay\")\n",
        "    parser.add_argument('--momentum', type=float, default=0.5,\n",
        "                        help='SGD momentum (default: 0.5)')\n",
        "    parser.add_argument('--tol', type=float, default=1,\n",
        "                        help='the maximum allowed difference between \\\n",
        "                        old and new clients accuracy')\n",
        "    parser.add_argument('--bias', type=float, default=1,\n",
        "                        help='A bias value between 0 and 1 that specifies the\\\n",
        "                         likelihood of selecting new clients during training')\n",
        "    parser.add_argument('--step_iter', type=int, default=1,\n",
        "                        help='to decide when the gen training should stop')\n",
        "    parser.add_argument('--average', type=int, default=1,\n",
        "                        help='initialization of the embedding vectors of the \\\n",
        "                        new clients as the average of the embedding vectors\\\n",
        "                        of the old clients')\n",
        "    parser.add_argument('--update', type=int, default=1,\n",
        "                        help='to decide if only the embedding weights should \\\n",
        "                        be updated')\n",
        "    parser.add_argument('--extra', type=float, default=0,\n",
        "                        help='The number of extra rounds of \\\n",
        "                        new clients fine tuning')\n",
        "\n",
        "\n",
        "    # model arguments\n",
        "    parser.add_argument('--model', type=str, default='cnn', help='model name')\n",
        "    parser.add_argument('--kernel_num', type=int, default=9,\n",
        "                        help='number of each kind of kernel')\n",
        "    parser.add_argument('--kernel_sizes', type=str, default='3,4,5',\n",
        "                        help='comma-separated kernel size to \\\n",
        "                        use for convolution')\n",
        "    parser.add_argument('--num_channels', type=int, default=1, help=\"number \\\n",
        "                        of channels of imgs\")\n",
        "    parser.add_argument('--norm', type=str, default='batch_norm',\n",
        "                        help=\"batch_norm, layer_norm, or None\")\n",
        "    parser.add_argument('--num_filters', type=int, default=32,\n",
        "                        help=\"number of filters for conv nets -- 32 for \\\n",
        "                        mini-imagenet, 64 for omiglot.\")\n",
        "    parser.add_argument('--max_pool', type=str, default='True',\n",
        "                        help=\"Whether use max pooling rather than \\\n",
        "                        strided convolutions\")\n",
        "    parser.add_argument('--checkpoint_resume', type=int, default=0,\n",
        "                        help='resume from checkpoint, 0 for False, 1 for True')\n",
        "    parser.add_argument(\"--inner-wd\", type=float, default=5e-5,\n",
        "                        help=\"inner weight decay\")\n",
        "    parser.add_argument(\"--embed-dim\", type=int, default=-1,\n",
        "                        help=\"embedding dim\")\n",
        "    parser.add_argument(\"--embed-lr\", type=float, default=None,\n",
        "                        help=\"embedding learning rate\")\n",
        "    parser.add_argument(\"--hyper-hid\", type=int, default=100,\n",
        "                        help=\"hypernet hidden dim\")\n",
        "    parser.add_argument(\"--spec-norm\", type=str2bool, default=False,\n",
        "                        help=\"hypernet hidden dim\")\n",
        "    parser.add_argument('--algorithm', type=str, default='pFedHN', help='Default set to FedAvg.')\n",
        "\n",
        "\n",
        "    # other arguments\n",
        "    parser.add_argument('--dataset', type=str, default='cifar', help=\"name \\\n",
        "                        of dataset\")\n",
        "    parser.add_argument('--val_split', type=float, default=0.2,\n",
        "                        help=\"train-validation split\")\n",
        "    parser.add_argument('--num_classes', type=int, default=100, help=\"number \\\n",
        "                        of classes\")\n",
        "    parser.add_argument('--gpu', default=None, help=\"To use cuda, set \\\n",
        "                        to a specific GPU ID. Default set to use CPU.\")\n",
        "    parser.add_argument('--optimizer', type=str, default='sgd', help=\"type \\\n",
        "                        of optimizer\")\n",
        "    parser.add_argument('--iid', type=int, default=0,\n",
        "                        help='Default set to IID. Set to 0 for non-IID.')\n",
        "    parser.add_argument('--participation', type=int, default=1,\n",
        "                        help='Default set to Uniform Participation. Set to 0 for Skewed')\n",
        "    parser.add_argument('--backup', type=int, default=500,\n",
        "                        help='How often an old backup should be preserved')\n",
        "    parser.add_argument('--checkpoint_path', type=str, default=\".\",\n",
        "                        help='Saved models location')\n",
        "    parser.add_argument('--stopping_rounds', type=int, default=10,\n",
        "                        help='rounds of early stopping')\n",
        "    parser.add_argument('--print_every', type=int, default=10,\n",
        "                        help='how often the train_accuracy is computed, and \\\n",
        "                        how often a new checkpoint is saved')\n",
        "    parser.add_argument('--verbose', type=int, default=0, help='verbose')\n",
        "    parser.add_argument('--gamma', type=float, default=0.1, help='gamma')\n",
        "    parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
        "    parser.add_argument('--metrics_dir', type=str, default='/content/drive/MyDrive/MLDL/cifar/metrics', help='metrics directory')\n",
        "\n",
        "    # KNN arguments\n",
        "    parser.add_argument(\n",
        "            '--interpolate_logits',\n",
        "            help='if selected logits are interpolated instead of probabilities',\n",
        "            default=\"store_true\"\n",
        "        )\n",
        "\n",
        "\n",
        "    # If running in a notebook, ignore the first argument which is the script name\n",
        "    args = parser.parse_args(args=sys.argv[1:] if \"__file__\" in globals() else [])\n",
        "    return args\n"
      ],
      "metadata": {
        "id": "pJSDrBVEpAty"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Client Class"
      ],
      "metadata": {
        "id": "TECc9agfRilQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from collections import Counter\n",
        "\n",
        "class Client:\n",
        "    def __init__(self, args, client_id, train_dataset, test_dataset, train_indices, val_indices, test_indices):\n",
        "        self.client_id = client_id\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.train_indices = train_indices\n",
        "        self.val_indices = val_indices\n",
        "        self.test_indices = test_indices\n",
        "        self.batch_size = args.local_bs\n",
        "        self.train_dataloader = self.create_dataloader(\"train\")\n",
        "        self.val_dataloader = self.create_dataloader(\"val\")\n",
        "        self.test_dataloader = self.create_dataloader(\"test\")\n",
        "    def get_class_distribution(self, indices, dataset):\n",
        "        targets = [dataset.targets[idx] for idx in indices]\n",
        "        return dict(Counter(targets))\n",
        "\n",
        "    def get_distributions(self):\n",
        "        train_dist = self.get_class_distribution(self.train_indices, self.train_dataset)\n",
        "        val_dist = self.get_class_distribution(self.val_indices, self.train_dataset)\n",
        "        test_dist = self.get_class_distribution(self.test_indices, self.test_dataset)\n",
        "\n",
        "        return {\n",
        "            'train': train_dist,\n",
        "            'val': val_dist,\n",
        "            'test': test_dist\n",
        "        }\n",
        "\n",
        "    def create_dataloader(self, dataset_type):\n",
        "        dataset_dict = {\n",
        "            \"train\": (self.train_dataset, self.train_indices),\n",
        "            \"val\": (self.train_dataset, self.val_indices),\n",
        "            \"test\": (self.test_dataset, self.test_indices)\n",
        "        }\n",
        "\n",
        "        dataset, indices = dataset_dict[dataset_type]\n",
        "        subset = Subset(dataset, indices)\n",
        "        dataloader = DataLoader(subset, batch_size=self.batch_size, shuffle=True)\n",
        "        return dataloader\n",
        "\n",
        "    def print_class_distribution(self):\n",
        "        def get_class_distribution(indices, dataset):\n",
        "            targets = [dataset.targets[idx] for idx in indices]\n",
        "            return dict(Counter(targets))\n",
        "\n",
        "        train_dist = get_class_distribution(self.train_indices, self.train_dataset)\n",
        "        val_dist = get_class_distribution(self.val_indices, self.train_dataset)\n",
        "        test_dist = get_class_distribution(self.test_indices, self.test_dataset)\n",
        "\n",
        "        print(f\"Client {self.client_id} class distribution:\")\n",
        "        print(f\"  Train: {train_dist}\")\n",
        "        print(f\"  Val: {val_dist}\")\n",
        "        print(f\"  Test: {test_dist}\")\n",
        "\n",
        "    def print_class_distribution(self):\n",
        "        def get_class_distribution(indices, dataset):\n",
        "            targets = [dataset.targets[idx] for idx in indices]\n",
        "            return dict(Counter(targets))\n",
        "\n",
        "        train_dist = get_class_distribution(self.train_indices, self.train_dataset)\n",
        "        val_dist = get_class_distribution(self.val_indices, self.train_dataset)\n",
        "        test_dist = get_class_distribution(self.test_indices, self.test_dataset)\n",
        "\n",
        "        print(f\"Client {self.client_id} class distribution:\")\n",
        "        print(f\"  Train: {train_dist}\")\n",
        "        print(f\"  Val: {val_dist}\")\n",
        "        print(f\"  Test: {test_dist}\")\n",
        "\n",
        "    def check_indices(self):\n",
        "        def has_duplicates(lst):\n",
        "            return len(lst) != len(set(lst))\n",
        "\n",
        "        if has_duplicates(self.train_indices):\n",
        "            raise ValueError(\"Duplicate entries found in train_indices\")\n",
        "        if has_duplicates(self.val_indices):\n",
        "            raise ValueError(\"Duplicate entries found in val_indices\")\n",
        "        if has_duplicates(self.test_indices):\n",
        "            raise ValueError(\"Duplicate entries found in test_indices\")\n",
        "\n",
        "        train_indices_set = set(self.train_indices)\n",
        "        val_indices_set = set(self.val_indices)\n",
        "\n",
        "        if not train_indices_set.isdisjoint(val_indices_set):\n",
        "            raise ValueError(\"Overlap found between train_indices and val_indices\")\n",
        "        if not val_indices_set.isdisjoint(train_indices_set):\n",
        "            raise ValueError(\"Overlap found between val_indices and train_indices\")\n",
        "\n",
        "    def train(self, model, criterion, optimizer, args):\n",
        "        self.train_dataloader = self.create_dataloader('train')  # Recreate dataloader to shuffle data\n",
        "\n",
        "        model.train()\n",
        "        step_count = 0  # Initialize step counter\n",
        "        while step_count < args.local_ep:  # Loop until local steps are reached\n",
        "            for inputs, labels in self.train_dataloader:\n",
        "                if args.device == 'cuda':\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                step_count += 1\n",
        "                if step_count >= args.local_ep:  # Exit if local steps are reached\n",
        "                    break\n",
        "        return model\n",
        "\n",
        "    def inference(self, model, criterion, args, loader_type='test'):\n",
        "        model.eval()\n",
        "        correct, total, test_loss = 0.0, 0.0, 0.0\n",
        "        testloader = self.test_dataloader if loader_type == 'test' else self.val_dataloader\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
        "                if args.device == 'cuda':\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        test_loss = test_loss / len(testloader)\n",
        "        accuracy = correct / total\n",
        "        return accuracy, test_loss\n",
        "\n",
        "    def single_batch_inference(self, model, criterion, args): # Ahmad his function is not necessary atm\n",
        "        model.eval()\n",
        "        testloader = iter(self.test_dataloader)\n",
        "        with torch.no_grad():\n",
        "            inputs, labels = next(testloader)\n",
        "            if args.device == 'cuda':\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct = (predicted == labels).sum().item()\n",
        "            accuracy = correct / len(labels)\n",
        "        return accuracy, loss.item()\n",
        "\n",
        "\n",
        "def cifar_iid(args, train_dataset, test_dataset):\n",
        "    val_split = args.val_split\n",
        "    num_clients = args.num_users\n",
        "\n",
        "    # Number of classes in the dataset\n",
        "    num_classes = len(train_dataset.classes)\n",
        "\n",
        "    # Create a list to store indices for each class\n",
        "    class_indices = [[] for _ in range(num_classes)]\n",
        "\n",
        "    # Populate class_indices with the indices of each class\n",
        "    for idx, target in enumerate(train_dataset.targets):\n",
        "        class_indices[target].append(idx)\n",
        "\n",
        "    # Shuffle indices within each class\n",
        "    for indices in class_indices:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    # Create lists for train and validation class indices\n",
        "    train_class_indices = [[] for _ in range(num_classes)]\n",
        "    val_class_indices = [[] for _ in range(num_classes)]\n",
        "\n",
        "    # Split the indices into 80% for train and 20% for validation\n",
        "    for i, indices in enumerate(class_indices):\n",
        "        split_idx = int(len(indices) * val_split)\n",
        "        val_class_indices[i] = indices[:split_idx]\n",
        "        train_class_indices[i] = indices[split_idx:]\n",
        "\n",
        "    # Prepare test_class_indices\n",
        "    test_class_indices = [[] for _ in range(num_classes)]\n",
        "    for idx, target in enumerate(test_dataset.targets):\n",
        "        test_class_indices[target].append(idx)\n",
        "    for indices in test_class_indices:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    # Calculate the number of samples per client per class\n",
        "    train_samples_per_client_per_class = int(len(train_dataset) * (1-val_split) // (num_clients * num_classes))\n",
        "    val_samples_per_client_per_class = int(len(train_dataset) * val_split // (num_clients * num_classes))\n",
        "    test_samples_per_client_per_class = len(test_dataset) // (num_clients * num_classes)\n",
        "\n",
        "    # Initialize the list of client objects\n",
        "    clients = []\n",
        "\n",
        "    # Distribute the samples uniformly to the clients\n",
        "    for client_id in range(num_clients):\n",
        "        train_client_indices = []\n",
        "        val_client_indices = []\n",
        "        test_client_indices = []\n",
        "\n",
        "        for train_class_indices_for_class in train_class_indices:\n",
        "            train_client_indices.extend(train_class_indices_for_class[client_id * train_samples_per_client_per_class : (client_id + 1) * train_samples_per_client_per_class])\n",
        "        for val_class_indices_for_class in val_class_indices:\n",
        "            val_client_indices.extend(val_class_indices_for_class[client_id * val_samples_per_client_per_class : (client_id + 1) * val_samples_per_client_per_class])\n",
        "        for test_class_indices_for_class in test_class_indices:\n",
        "            test_client_indices.extend(test_class_indices_for_class[client_id * test_samples_per_client_per_class : (client_id + 1) * test_samples_per_client_per_class])\n",
        "\n",
        "        client = Client(args, client_id, train_dataset, test_dataset, train_client_indices, val_client_indices, test_client_indices)\n",
        "        clients.append(client)\n",
        "\n",
        "    return clients\n",
        "\n",
        "def cifar_noniid(args, train_dataset, test_dataset):\n",
        "    def class_clients_sharding(num_classes, Nc):\n",
        "        class_clients = {key: set() for key in range(num_classes)}\n",
        "        first_clients = list(range(num_classes))\n",
        "        clients_list = [num // (Nc-1) for num in range((Nc-1)*100)]\n",
        "        random.shuffle(first_clients)\n",
        "        for i in range(num_classes):\n",
        "            class_clients[i].add(first_clients[i])\n",
        "\n",
        "        for j in range(1,Nc):\n",
        "            class_list = list(range(num_classes))\n",
        "            for i in range(num_classes):\n",
        "                random_class = random.choice(class_list)\n",
        "                class_list.remove(random_class)\n",
        "\n",
        "                clients_list_cleaned = [client for client in clients_list if client not in class_clients[random_class]]\n",
        "\n",
        "                random_client = random.choice(clients_list_cleaned)\n",
        "                class_clients[random_class].add(random_client)\n",
        "                clients_list.remove(random_client)\n",
        "\n",
        "        return class_clients\n",
        "\n",
        "    val_split = args.val_split\n",
        "    num_clients = args.num_users\n",
        "    Nc = args.Nc\n",
        "    num_classes = len(train_dataset.classes)\n",
        "\n",
        "    error = True\n",
        "    while error:\n",
        "        try:\n",
        "            class_clients = class_clients_sharding(num_classes, Nc)\n",
        "            error = False\n",
        "        except Exception as e:\n",
        "            print(\"Sharding Invalid, trying again...\")\n",
        "\n",
        "    # Create a list to store indices for each class\n",
        "    class_indices = [[] for _ in range(num_classes)]\n",
        "\n",
        "    # Populate class_indices with the indices of each class\n",
        "    for idx, target in enumerate(train_dataset.targets):\n",
        "        class_indices[target].append(idx)\n",
        "\n",
        "    # Shuffle indices within each class\n",
        "    for indices in class_indices:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    # Create lists for train and validation class indices\n",
        "    train_class_indices = [[] for _ in range(num_classes)]\n",
        "    val_class_indices = [[] for _ in range(num_classes)]\n",
        "\n",
        "    # Split the indices into 80% for train and 20% for validation\n",
        "    for i, indices in enumerate(class_indices):\n",
        "        split_idx = int(len(indices) * val_split)\n",
        "        val_class_indices[i] = indices[:split_idx]\n",
        "        train_class_indices[i] = indices[split_idx:]\n",
        "\n",
        "    # Prepare test_class_indices\n",
        "    test_class_indices = [[] for _ in range(num_classes)]\n",
        "    for idx, target in enumerate(test_dataset.targets):\n",
        "        test_class_indices[target].append(idx)\n",
        "    for indices in test_class_indices:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    # Initialize the list of client objects\n",
        "    clients_list = []\n",
        "\n",
        "    # Calculate the number of samples per client per class\n",
        "    train_samples_per_client_per_class = int(len(train_dataset) * (1-val_split) // (Nc * num_classes))\n",
        "    val_samples_per_client_per_class = int(len(train_dataset) * val_split // (Nc * num_classes))\n",
        "    test_samples_per_client_per_class = len(test_dataset) // (Nc * num_classes)\n",
        "\n",
        "\n",
        "    train_shards_indices = [[] for clients in range(num_clients)]\n",
        "    val_shards_indices = [[] for clients in range(num_clients)]\n",
        "    test_shards_indices = [[] for clients in range(num_clients)]\n",
        "\n",
        "    for class_idx in range(num_classes):\n",
        "        train_class_indices_for_class = train_class_indices[class_idx]\n",
        "        val_class_indices_for_class = val_class_indices[class_idx]\n",
        "        test_class_indices_for_class = test_class_indices[class_idx]\n",
        "        clients = class_clients[class_idx].copy()\n",
        "        for client_idx in range(Nc):\n",
        "            client = random.choice(list(clients))\n",
        "            clients.remove(client)\n",
        "\n",
        "            train_start_idx = client_idx * int(train_samples_per_client_per_class)\n",
        "            val_start_idx = client_idx * int(val_samples_per_client_per_class)\n",
        "            test_start_idx = client_idx * int(test_samples_per_client_per_class)\n",
        "\n",
        "            train_end_idx = (client_idx + 1) * int(train_samples_per_client_per_class)\n",
        "            val_end_idx = (client_idx + 1) * int(val_samples_per_client_per_class)\n",
        "            test_end_idx = (client_idx + 1) * int(test_samples_per_client_per_class)\n",
        "\n",
        "            train_shards_indices[client].extend(train_class_indices_for_class[train_start_idx:train_end_idx])\n",
        "            val_shards_indices[client].extend(val_class_indices_for_class[val_start_idx:val_end_idx])\n",
        "            test_shards_indices[client].extend(test_class_indices_for_class[test_start_idx:test_end_idx])\n",
        "\n",
        "    for client_id in range(num_clients):\n",
        "        client = Client(args, client_id, train_dataset, test_dataset, train_shards_indices[client_id], val_shards_indices[client_id], test_shards_indices[client_id])\n",
        "        clients_list.append(client)\n",
        "\n",
        "    return clients_list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oPO3Q7k9zvET"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drive And Get Dataset"
      ],
      "metadata": {
        "id": "XRYpzjemfpHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "LYd7gt7PN5WX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076d4048-6ac7-484e-83e5-a11d6cf88357"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(filename):\n",
        "    if os.path.isfile(filename):\n",
        "        #checkpoint = torch.load(filename)\n",
        "        checkpoint = torch.load(filename, map_location=torch.device('cpu'))\n",
        "        print(f\"Loading checkpoint '{filename}' (epoch {checkpoint['epoch']})\")\n",
        "        return checkpoint\n",
        "    else:\n",
        "        print(f\"No checkpoint found at '{filename}'\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "lM_SPfRVOAGQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(args):\n",
        "    \"\"\" Returns train and test datasets and a user group which is a dict where\n",
        "    the keys are the user index and the values are the corresponding data for\n",
        "    each of those users.\n",
        "    \"\"\"\n",
        "    # data_dir = args.data_dir\n",
        "    data_dir = \"/content/drive/MyDrive/MLDL/\"\n",
        "    if args.dataset == 'cifar':\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "        ])\n",
        "        train_dataset = datasets.CIFAR100(data_dir, train=True, download=True,\n",
        "                                       transform=transform_train)\n",
        "        test_dataset = datasets.CIFAR100(data_dir, train=False, download=True,\n",
        "                                      transform=transform_test)\n",
        "        if args.iid:\n",
        "            clients = cifar_iid(args, train_dataset, test_dataset)\n",
        "        else:\n",
        "            clients = cifar_noniid(args, train_dataset, test_dataset)\n",
        "\n",
        "    return train_dataset, test_dataset, clients\n"
      ],
      "metadata": {
        "id": "_TGgWvcDz4zw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "ZVukJg2QfxbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_pfedhn(nodes, num_nodes, hnet, net, criterion, device, loader_type):\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(nodes, num_nodes, hnet, net, criterion, device, loader_type):\n",
        "        hnet.eval()\n",
        "        results = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "        for node_id in range(num_nodes):  # iterating over nodes\n",
        "            running_loss, running_correct, running_samples = 0., 0., 0.\n",
        "            if loader_type == 'test':\n",
        "                curr_data = nodes[node_id].test_dataloader\n",
        "            elif loader_type == 'val':\n",
        "                curr_data = nodes[node_id].val_dataloader\n",
        "            else:\n",
        "                curr_data = nodes[node_id].train_dataloader\n",
        "\n",
        "            for batch_count, batch in enumerate(curr_data):\n",
        "                img, label = tuple(t.to(device) for t in batch)\n",
        "\n",
        "                weights = hnet(torch.tensor([node_id], dtype=torch.long).to(device))\n",
        "                net.load_state_dict(weights)\n",
        "                pred = net(img)\n",
        "                running_loss += criterion(pred, label).item()\n",
        "                running_correct += pred.argmax(1).eq(label).sum().item()\n",
        "                running_samples += len(label)\n",
        "\n",
        "            results[node_id]['loss'] = running_loss / (batch_count + 1)\n",
        "            results[node_id]['correct'] = running_correct\n",
        "            results[node_id]['total'] = running_samples\n",
        "            results[node_id]['accuracy'] = running_correct / running_samples\n",
        "\n",
        "        return results\n",
        "\n",
        "    curr_results = evaluate(nodes, num_nodes, hnet, net, criterion, device, loader_type=loader_type)\n",
        "    total_correct = sum([val['correct'] for val in curr_results.values()])\n",
        "    total_samples = sum([val['total'] for val in curr_results.values()])\n",
        "\n",
        "    avg_loss = np.mean([val['loss'] for val in curr_results.values()])\n",
        "    avg_acc = np.mean([val['accuracy'] for val in curr_results.values()])\n",
        "    acc = total_correct / total_samples\n",
        "\n",
        "    return curr_results, avg_loss, avg_acc, acc"
      ],
      "metadata": {
        "id": "MurSs6rBe4ny"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_pfedhn_gen(nodes, num_nodes, num_users, hnet, net, criterion, device, loader_type):\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(nodes, num_nodes, num_users, hnet, net, criterion, device, loader_type):\n",
        "        hnet.eval()\n",
        "        results = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "        for node_id in range(num_users):  # iterating over nodes\n",
        "            running_loss, running_correct, running_samples = 0., 0., 0.\n",
        "            if loader_type == 'test':\n",
        "                curr_data = nodes[node_id].test_dataloader\n",
        "            elif loader_type == 'val':\n",
        "                curr_data = nodes[node_id].val_dataloader\n",
        "            else:\n",
        "                curr_data = nodes[node_id].train_dataloader\n",
        "\n",
        "            for batch_count, batch in enumerate(curr_data):\n",
        "                img, label = tuple(t.to(device) for t in batch)\n",
        "\n",
        "                weights = hnet(torch.tensor([node_id], dtype=torch.long).to(device))\n",
        "                net.load_state_dict(weights)\n",
        "                pred = net(img)\n",
        "                running_loss += criterion(pred, label).item()\n",
        "                running_correct += pred.argmax(1).eq(label).sum().item()\n",
        "                running_samples += len(label)\n",
        "\n",
        "            results[node_id]['loss'] = running_loss / (batch_count + 1)\n",
        "            results[node_id]['correct'] = running_correct\n",
        "            results[node_id]['total'] = running_samples\n",
        "            results[node_id]['accuracy'] = running_correct / running_samples\n",
        "\n",
        "        return results\n",
        "\n",
        "    curr_results = evaluate(nodes, num_nodes, num_users, hnet, net, criterion, device, loader_type=loader_type)\n",
        "    total_correct = sum([val['correct'] for val in curr_results.values()])\n",
        "    total_samples = sum([val['total'] for val in curr_results.values()])\n",
        "\n",
        "    avg_loss_new = np.mean([curr_results[node_id]['loss'] for node_id in range(num_nodes, num_users)])\n",
        "    avg_acc_new = np.mean([curr_results[node_id]['accuracy'] for node_id in range(num_nodes, num_users)])\n",
        "    loss = np.mean([val['loss'] for val in curr_results.values()])\n",
        "    acc = np.mean([val['accuracy'] for val in curr_results.values()])\n",
        "\n",
        "    return curr_results, avg_loss_new, avg_acc_new, loss, acc"
      ],
      "metadata": {
        "id": "sRLhnaXZi9ti"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypernetwork"
      ],
      "metadata": {
        "id": "ecZwFyb1gmg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pFedHN(global_model, clients, criterion, args, metrics, device, test_set):\n",
        "    nodes = clients\n",
        "    clients_distribs = {client.client_id: 0 for client in clients}\n",
        "    embed_dim = args.embed_dim\n",
        "    num_nodes = args.n_nodes\n",
        "\n",
        "    if embed_dim == -1:\n",
        "        embed_dim = int(1 + num_nodes / 4)\n",
        "\n",
        "    # Ahmad add a check if there is a saving checkpoint if it there is then load the state dict on the global_model and the hypernetwork\n",
        "    hnet = CNNHyper(num_nodes, embed_dim).to(device)\n",
        "    net = global_model\n",
        "\n",
        "    ##################\n",
        "    # init optimizer #\n",
        "    ##################\n",
        "    lr = args.lr\n",
        "    embed_lr = args.embed_lr\n",
        "    wd = args.wd\n",
        "\n",
        "    embed_lr = embed_lr if embed_lr is not None else lr\n",
        "    optimizers = {\n",
        "        'sgd': torch.optim.SGD(\n",
        "            [\n",
        "                {'params': [p for n, p in hnet.named_parameters() if 'embed' not in n]},\n",
        "                {'params': [p for n, p in hnet.named_parameters() if 'embed' in n], 'lr': embed_lr}\n",
        "            ], lr=lr, weight_decay=wd\n",
        "        )\n",
        "    }\n",
        "    optimizer = optimizers[args.optimizer]\n",
        "\n",
        "    ################\n",
        "    # init metrics #\n",
        "    ################\n",
        "    results = defaultdict(list)\n",
        "    dirichlet_probs = np.random.dirichlet([args.gamma] * num_nodes)\n",
        "\n",
        "    step_iter = trange(args.epochs)\n",
        "    for step in step_iter:\n",
        "        hnet.train()\n",
        "\n",
        "        if args.participation:\n",
        "            # Uniform participation\n",
        "            node_id = np.random.choice(range(num_nodes))\n",
        "        else:\n",
        "            # Skewed participation\n",
        "            node_id = np.random.choice(range(num_nodes), p=dirichlet_probs)\n",
        "\n",
        "        clients_distribs[node_id] = 1\n",
        "\n",
        "        # produce & load local network weights\n",
        "        weights = hnet(torch.tensor([node_id], dtype=torch.long).to(device))\n",
        "        net.load_state_dict(weights)\n",
        "\n",
        "        # init inner optimizer\n",
        "        inner_optim = torch.optim.SGD(\n",
        "            net.parameters(), lr=args.lr, weight_decay=args.inner_wd\n",
        "        )\n",
        "\n",
        "        # storing theta_i for later calculating delta theta\n",
        "        inner_state = OrderedDict({k: tensor.data for k, tensor in weights.items()})\n",
        "\n",
        "        # inner updates -> obtaining theta_tilda\n",
        "        inner_steps = args.local_ep * 10\n",
        "        for i in range(inner_steps):\n",
        "            net.train()\n",
        "            inner_optim.zero_grad()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch = next(iter(nodes[node_id].train_dataloader))\n",
        "            img, label = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            pred = net(img)\n",
        "\n",
        "            loss = criterion(pred, label)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), 50)\n",
        "\n",
        "            inner_optim.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        final_state = net.state_dict()\n",
        "\n",
        "        # calculating delta theta\n",
        "        delta_theta = OrderedDict({k: inner_state[k] - final_state[k] for k in weights.keys()})\n",
        "\n",
        "        # calculating phi gradient\n",
        "        hnet_grads = torch.autograd.grad(\n",
        "            list(weights.values()), hnet.parameters(), grad_outputs=list(delta_theta.values())\n",
        "        )\n",
        "\n",
        "        # update hnet weights\n",
        "        for p, g in zip(hnet.parameters(), hnet_grads):\n",
        "            p.grad = g\n",
        "        torch.nn.utils.clip_grad_norm_(hnet.parameters(), 50)\n",
        "        optimizer.step()\n",
        "\n",
        "        # logger.info(f\"\\n\\nStep: {step+1}, Node ID: {node_id}, Loss: {prvs_loss:.4f},  Acc: {prvs_acc:.4f}\")\n",
        "        if (step +1) % args.print_every == 0:\n",
        "            filename = f\"{args.checkpoint_path}/checkpoint_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}_epoch_{step+1}.pth.tar\"\n",
        "\n",
        "            last_eval = step\n",
        "            step_results, test_avg_loss, test_avg_acc, test_acc = eval_pfedhn(nodes, num_nodes, hnet, net, criterion, device, loader_type=\"test\")\n",
        "\n",
        "            print(f\"\\nStep: {step+1}, AVG Test Loss: {test_avg_loss:.4f},  AVG Test Acc: {test_avg_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "            results['test_avg_loss'].append(test_avg_loss)\n",
        "            results['test_avg_acc'].append(test_avg_acc)\n",
        "            results['test_acc'].append(test_acc)\n",
        "\n",
        "            _, val_avg_loss, val_avg_acc, val_acc  = eval_pfedhn(nodes, num_nodes, hnet, net, criterion, device, loader_type=\"val\")\n",
        "\n",
        "            results['val_avg_loss'].append(val_avg_loss)\n",
        "            results['val_avg_acc'].append(val_avg_acc)\n",
        "            results['val_acc'].append(val_acc)\n",
        "\n",
        "            for key in results:\n",
        "                print(f\"{key}: {results[key][-1]}\")\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': step + 1,\n",
        "                'model_state_dict': global_model.state_dict(),\n",
        "                'hn_state_dict': hnet.state_dict(),\n",
        "                'user_input': (args.iid, args.participation, args.Nc, args.local_ep),\n",
        "                'test_accuracy': results['test_acc'][-1],\n",
        "                'test_avg_loss': results['test_avg_loss'][-1],\n",
        "                'test_avg_acc': results['test_avg_acc'][-1],\n",
        "                'val_accuracy': results['val_acc'][-1],\n",
        "                'val_avg_loss': results['val_avg_loss'][-1],\n",
        "                'val_avg_acc': results['val_avg_acc'][-1]\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=filename)\n",
        "\n",
        "            # Remove the previous checkpoint unless it's a multiple of the backup parameter\n",
        "            prev_epoch = step + 1 - args.print_every\n",
        "            if (step + 1) > args.print_every and prev_epoch != 1900:\n",
        "                if (step + 1 -10) % args.backup != 0:\n",
        "                    prev_filename = f\"{args.checkpoint_path}/checkpoint_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}_epoch_{prev_epoch}.pth.tar\"\n",
        "\n",
        "                    if os.path.exists(prev_filename):\n",
        "                        os.remove(prev_filename)\n",
        "            # metrics = pd.DataFrame(columns=['Round', 'Test Accuracy', 'Test Loss', 'Avg Test Accuracy', 'Avg Test Loss', 'Avg Validation Accuracy', 'Avg Validation Loss'])\n",
        "\n",
        "            metrics.loc[len(metrics)] = [step + 1, results['test_acc'][-1], results['test_avg_loss'][-1], results['test_avg_acc'][-1], results['val_acc'][-1], results['val_avg_loss'][-1], results['val_avg_acc'][-1]]\n",
        "    if step != last_eval:\n",
        "        _, val_avg_loss, val_avg_acc, val_acc  = eval_pfedhn(nodes, num_nodes, hnet, net, criterion, device, loader_type=\"val\")\n",
        "\n",
        "        results['test_avg_loss'].append(test_avg_loss)\n",
        "        results['test_avg_acc'].append(test_avg_acc)\n",
        "        results['test_acc'].append(test_acc)\n",
        "\n",
        "        step_results, test_avg_loss, test_avg_acc, test_acc = eval_pfedhn(nodes, num_nodes, hnet, net, criterion, device, loader_type=\"test\")\n",
        "        print(f\"\\nStep: {step+1}, AVG Test Loss: {test_avg_loss:.4f},  AVG Test Acc: {test_avg_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "        results['val_avg_loss'].append(val_avg_loss)\n",
        "        results['val_avg_acc'].append(val_avg_acc)\n",
        "        results['val_acc'].append(val_acc)\n",
        "\n",
        "    # Save the plot as a PDF file\n",
        "    if args.participation:\n",
        "        pickle_file = f\"{args.metrics_dir}/metrics_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}.pkl\"\n",
        "    else:\n",
        "        pickle_file = f\"{args.metrics_dir}/metrics_{args.algorithm}_{args.iid}_{args.participation}_{args.gamma}_{args.Nc}_{args.local_ep}.pkl\"\n",
        "\n",
        "\n",
        "    # Optionally, clear the figure to free up memory\n",
        "\n",
        "\n",
        "    metrics.to_pickle(pickle_file)\n",
        "    logger.info(f\"Metrics saved at {pickle_file}\")\n",
        "    logger.info(f\"Plots saved at {plot_location}\")\n",
        "    logger.info(\"Training Done!\")"
      ],
      "metadata": {
        "id": "Kvd9Tffkimsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypernetwork With Checkpoint"
      ],
      "metadata": {
        "id": "joP97e2BgIFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pFedHN(global_model, clients, criterion, args, metrics, device, test_set):\n",
        "    nodes = clients\n",
        "    clients_distribs = {client.client_id: 0 for client in clients}\n",
        "    embed_dim = args.embed_dim\n",
        "    num_nodes = args.n_nodes\n",
        "\n",
        "    # Print the ID and indices of the first 5 clients\n",
        "    for i in range(min(5, len(clients))):  # To ensure we only print if there are at least 5 clients\n",
        "        print(f\"Client ID: {clients[i].client_id}\")\n",
        "        print(f\"Train indices: {clients[i].train_indices[:10]}\")  # Print only the first 10 indices for brevity\n",
        "        print(f\"Val indices: {clients[i].val_indices[:10]}\")  # Print only the first 10 indices\n",
        "        print(f\"Test indices: {clients[i].test_indices[:10]}\")  # Print only the first 10 indices\n",
        "        print('-' * 50)\n",
        "\n",
        "    if embed_dim == -1:\n",
        "        embed_dim = int(1 + num_nodes / 4)\n",
        "\n",
        "    # Ahmad add a check if there is a saving checkpoint if it there is then load the state dict on the global_model and the hypernetwork\n",
        "    hnet = CNNHyper(num_nodes, embed_dim).to(device)\n",
        "    net = global_model\n",
        "\n",
        "    last_epoch = args.last_epoch\n",
        "    #last_epoch = 1900\n",
        "    if args.participation:\n",
        "        checkpoint_pattern = f\"{args.checkpoint_path}/checkpoint_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}_epoch_{args.last_epoch}.pth.tar\"\n",
        "    else:\n",
        "        checkpoint_pattern = f\"{args.checkpoint_path}/checkpoint_{args.algorithm}_{args.iid}_{args.participation}_{args.gamma}_{args.Nc}_{args.local_ep}_epoch_{args.last_epoch}.pth.tar\"\n",
        "    checkpoint_files = sorted(glob.glob(checkpoint_pattern))\n",
        "    print(checkpoint_pattern)\n",
        "    print(\"len:\",len(checkpoint_files))\n",
        "\n",
        "    if len(checkpoint_files)+1:\n",
        "        latest_checkpoint = checkpoint_files[-1]\n",
        "        checkpoint = load_checkpoint(latest_checkpoint)\n",
        "        print(\"\\nciao mamma\")\n",
        "        if checkpoint:\n",
        "            print(\"\\n ciao papa\")\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            last_user_input = checkpoint['user_input']\n",
        "            test_acc = checkpoint['test_accuracy']\n",
        "            test_avg_loss = checkpoint['test_avg_loss']\n",
        "            test_avg_acc = checkpoint['test_avg_acc']\n",
        "            val_acc = checkpoint['val_accuracy']\n",
        "            val_avg_loss = checkpoint['val_avg_loss']\n",
        "            val_avg_acc = checkpoint['val_avg_acc']\n",
        "            # Print the status of the last checkpoint\n",
        "            participation_status = 'uniform' if last_user_input[1] == 1 else 'skewed'\n",
        "            user_input_string = f\"IID: {last_user_input[0]}, Participation: {participation_status}, Nc: {last_user_input[2]}, J: {last_user_input[3]}\"\n",
        "\n",
        "            #logger.info(f\"\\nA saving checkpoint with these parameters exists:\\n\"\n",
        "            print(f\"\\nA saving checkpoint with these parameters exists:\\n\"\n",
        "                f\"Last checkpoint details:\\n\"\n",
        "                f\"Epoch reached: {start_epoch}\\n\"\n",
        "                f\"User input variables: {user_input_string}\\n\"\n",
        "                f'Test Accuracy: {100*test_acc}%\\n'\n",
        "                f'Test Avg Loss: {test_avg_loss}\\n'\n",
        "                f'Test Avg Acc: {100*test_avg_acc}%\\n'\n",
        "                f'Val Accuracy: {100*val_acc}%\\n'\n",
        "                f'Val Avg Loss {val_avg_loss}\\n'\n",
        "                f'Val Avg Acc {100*val_avg_acc}%\\n')\n",
        "\n",
        "\n",
        "            # print(\"\\nitems:\\n\",checkpoint['hn_state_dict'].items())\n",
        "            if args.checkpoint_resume == 1:\n",
        "\n",
        "                global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                net = global_model\n",
        "\n",
        "                hnet = CNNHyper(num_nodes, embed_dim).to(device)\n",
        "                hnet.load_state_dict(checkpoint['hn_state_dict'])\n",
        "\n",
        "\n",
        "\n",
        "    ##################\n",
        "    # init optimizer #\n",
        "    ##################\n",
        "    lr = args.lr\n",
        "    embed_lr = args.embed_lr\n",
        "    wd = args.wd\n",
        "\n",
        "    embed_lr = embed_lr if embed_lr is not None else lr\n",
        "    optimizers = {\n",
        "        'sgd': torch.optim.SGD(\n",
        "            [\n",
        "                {'params': [p for n, p in hnet.named_parameters() if 'embed' not in n]},\n",
        "                {'params': [p for n, p in hnet.named_parameters() if 'embed' in n], 'lr': embed_lr}\n",
        "            ], lr=lr, weight_decay=wd\n",
        "        )\n",
        "    }\n",
        "    optimizer = optimizers[args.optimizer]\n",
        "\n",
        "    ################\n",
        "    # init metrics #\n",
        "    ################\n",
        "    results = defaultdict(list)\n",
        "    dirichlet_probs = np.random.dirichlet([args.gamma] * num_nodes)\n",
        "\n",
        "    step_iter = trange(args.epochs - args.last_epoch + args.extra)\n",
        "\n",
        "    for step in step_iter:\n",
        "        hnet.train()\n",
        "\n",
        "        if args.participation:\n",
        "            # Uniform participation\n",
        "            node_id = np.random.choice(range(num_nodes))\n",
        "        else:\n",
        "            # Skewed participation\n",
        "            node_id = np.random.choice(range(num_nodes), p=dirichlet_probs)\n",
        "\n",
        "        clients_distribs[node_id] = 1\n",
        "\n",
        "        # produce & load local network weights\n",
        "        weights = hnet(torch.tensor([node_id], dtype=torch.long).to(device))\n",
        "        net.load_state_dict(weights)\n",
        "\n",
        "        # init inner optimizer\n",
        "        inner_optim = torch.optim.SGD(\n",
        "            net.parameters(), lr=args.lr, weight_decay=args.inner_wd\n",
        "        )\n",
        "\n",
        "        # storing theta_i for later calculating delta theta\n",
        "        inner_state = OrderedDict({k: tensor.data for k, tensor in weights.items()})\n",
        "\n",
        "        # inner updates -> obtaining theta_tilda\n",
        "        inner_steps = args.local_ep * 10\n",
        "        for i in range(inner_steps):\n",
        "            net.train()\n",
        "            inner_optim.zero_grad()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch = next(iter(nodes[node_id].train_dataloader))\n",
        "            img, label = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            pred = net(img)\n",
        "\n",
        "            loss = criterion(pred, label)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), 50)\n",
        "\n",
        "            inner_optim.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        final_state = net.state_dict()\n",
        "\n",
        "        # calculating delta theta\n",
        "        delta_theta = OrderedDict({k: inner_state[k] - final_state[k] for k in weights.keys()})\n",
        "\n",
        "        # calculating phi gradient\n",
        "        hnet_grads = torch.autograd.grad(\n",
        "            list(weights.values()), hnet.parameters(), grad_outputs=list(delta_theta.values())\n",
        "        )\n",
        "\n",
        "        # update hnet weights\n",
        "        for p, g in zip(hnet.parameters(), hnet_grads):\n",
        "            p.grad = g\n",
        "        torch.nn.utils.clip_grad_norm_(hnet.parameters(), 50)\n",
        "        optimizer.step()\n",
        "\n",
        "        # logger.info(f\"\\n\\nStep: {step+1}, Node ID: {node_id}, Loss: {prvs_loss:.4f},  Acc: {prvs_acc:.4f}\")\n",
        "        if (step +1) % args.print_every == 0:\n",
        "            filename = f\"{args.checkpoint_path}/checkpoint_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}_epoch_{args.last_epoch+step+1}.pth.tar\"\n",
        "\n",
        "            last_eval = step\n",
        "            step_results, test_avg_loss, test_avg_acc, test_acc = eval_pfedhn(nodes, num_nodes, hnet, net, criterion, device, loader_type=\"test\")\n",
        "\n",
        "            print(f\"\\nStep: {step+1}, AVG Test Loss: {test_avg_loss:.4f},  AVG Test Acc: {test_avg_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "            results['test_avg_loss'].append(test_avg_loss)\n",
        "            results['test_avg_acc'].append(test_avg_acc)\n",
        "            results['test_acc'].append(test_acc)\n",
        "\n",
        "            _, val_avg_loss, val_avg_acc, val_acc  = eval_pfedhn(nodes, num_nodes, hnet, net, criterion, device, loader_type=\"val\")\n",
        "\n",
        "            results['val_avg_loss'].append(val_avg_loss)\n",
        "            results['val_avg_acc'].append(val_avg_acc)\n",
        "            results['val_acc'].append(val_acc)\n",
        "\n",
        "            for key in results:\n",
        "                print(f\"{key}: {results[key][-1]}\")\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': step + 1,\n",
        "                'model_state_dict': global_model.state_dict(),\n",
        "                'hn_state_dict': hnet.state_dict(),\n",
        "                'user_input': (args.iid, args.participation, args.Nc, args.local_ep),\n",
        "                'test_accuracy': results['test_acc'][-1],\n",
        "                'test_avg_loss': results['test_avg_loss'][-1],\n",
        "                'test_avg_acc': results['test_avg_acc'][-1],\n",
        "                'val_accuracy': results['val_acc'][-1],\n",
        "                'val_avg_loss': results['val_avg_loss'][-1],\n",
        "                'val_avg_acc': results['val_avg_acc'][-1]\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=filename)\n",
        "\n",
        "            hnet_old = CNNHyper_old(num_nodes, embed_dim).to(device)\n",
        "\n",
        "            # Load all other parameters except the embedding layer\n",
        "            hnet_state_dict = hnet.state_dict()\n",
        "            hnet_old.load_state_dict(checkpoint['hn_state_dict'])\n",
        "\n",
        "\n",
        "            print(\"\\n LOADED MODEL\\n\")\n",
        "            step_results, test_avg_loss, test_avg_acc, test_acc = eval_pfedhn(nodes, num_nodes, hnet_old, net, criterion, device, loader_type=\"test\")\n",
        "\n",
        "            print(f\"\\nStep: {step+1}, AVG Test Loss: {test_avg_loss:.4f},  AVG Test Acc: {test_avg_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Remove the previous checkpoint unless it's a multiple of the backup parameter\n",
        "            prev_epoch = step + 1 - args.print_every\n",
        "            if (step + 1) > args.print_every and prev_epoch != 2000:\n",
        "                if (step + 1 -10) % args.backup != 0:\n",
        "                    prev_filename = f\"{args.checkpoint_path}/checkpoint_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}_epoch_{prev_epoch}.pth.tar\"\n",
        "\n",
        "                    if os.path.exists(prev_filename):\n",
        "                        os.remove(prev_filename)\n",
        "            # metrics = pd.DataFrame(columns=['Round', 'Test Accuracy', 'Test Loss', 'Avg Test Accuracy', 'Avg Test Loss', 'Avg Validation Accuracy', 'Avg Validation Loss'])\n",
        "\n",
        "            metrics.loc[len(metrics)] = [step + 1, results['test_acc'][-1], results['test_avg_loss'][-1], results['test_avg_acc'][-1], results['val_acc'][-1], results['val_avg_loss'][-1], results['val_avg_acc'][-1]]\n",
        "    if step != last_eval:\n",
        "        _, val_avg_loss, val_avg_acc, val_acc  = eval_pfedhn(nodes, num_nodes, hnet, net, criterion, device, loader_type=\"val\")\n",
        "\n",
        "        results['test_avg_loss'].append(test_avg_loss)\n",
        "        results['test_avg_acc'].append(test_avg_acc)\n",
        "        results['test_acc'].append(test_acc)\n",
        "\n",
        "        step_results, test_avg_loss, test_avg_acc, test_acc = eval_pfedhn(nodes, num_nodes, hnet, net, criterion, device, loader_type=\"test\")\n",
        "        print(f\"\\nStep: {step+1}, AVG Test Loss: {test_avg_loss:.4f},  AVG Test Acc: {test_avg_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "        results['val_avg_loss'].append(val_avg_loss)\n",
        "        results['val_avg_acc'].append(val_avg_acc)\n",
        "        results['val_acc'].append(val_acc)\n",
        "\n",
        "    # Save the plot as a PDF file\n",
        "    if args.participation:\n",
        "        pickle_file = f\"{args.metrics_dir}/metrics_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}.pkl\"\n",
        "    else:\n",
        "        pickle_file = f\"{args.metrics_dir}/metrics_{args.algorithm}_{args.iid}_{args.participation}_{args.gamma}_{args.Nc}_{args.local_ep}.pkl\"\n",
        "\n",
        "\n",
        "    # Optionally, clear the figure to free up memory\n",
        "\n",
        "\n",
        "    metrics.to_pickle(pickle_file)\n",
        "    logger.info(f\"Metrics saved at {pickle_file}\")\n",
        "    logger.info(f\"Plots saved at {plot_location}\")\n",
        "    logger.info(\"Training Done!\")"
      ],
      "metadata": {
        "id": "MlCghNrXEeXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypernetwork Main"
      ],
      "metadata": {
        "id": "wfwL_pWngBjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from tqdm import trange\n",
        "import pickle\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TO DO: modify how things of args are set\n",
        "args = args_parser()\n",
        "args.epochs = 2000\n",
        "args.last_epoch = 1900\n",
        "args.iid = 0\n",
        "args.participation = 1\n",
        "args.algorithm = \"pfedhn\"\n",
        "args.Nc = 1\n",
        "args.local_ep = 4\n",
        "args.checkpoint_resume = 1\n",
        "args.checkpoint_path = \"/content/drive/MyDrive/MLDL/Cifar-100/Checkpoints\"\n",
        "if args.gpu:\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "device = 'cuda' if args.gpu else 'cpu'\n",
        "\n",
        "args.device = device\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "if args.dataset == 'cifar':\n",
        "    if args.algorithm == 'fedavg':\n",
        "        global_model = CIFARLeNet().to(device)\n",
        "    else:\n",
        "        global_model = CIFARLeNet().to(device)\n",
        "else:\n",
        "    global_model = CharLSTM().to(device)\n",
        "\n",
        "#train_set, test_set, clients = get_dataset(args)\n",
        "\n",
        "# TO DO: logger info\n",
        "\n",
        "\n",
        "if args.iid:\n",
        "    if args.participation:\n",
        "        pickle_file = f\"{args.metrics_dir}/clients_classes_dist_{args.algorithm}_{args.iid}_{args.participation}.pkl\"\n",
        "    else:\n",
        "        pickle_file = f\"{args.metrics_dir}/clients_classes_dist_{args.algorithm}_{args.iid}_{args.participation}_{args.gamma}.pkl\"\n",
        "else:\n",
        "    if args.participation:\n",
        "        pickle_file = f\"{args.metrics_dir}/clients_classes_dist_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}.pkl\"\n",
        "    else:\n",
        "        pickle_file = f\"{args.metrics_dir}/clients_classes_dist_{args.algorithm}_{args.iid}_{args.participation}_{args.gamma}_{args.Nc}_{args.local_ep}.pkl\"\n",
        "\n",
        "checkpoint_path = pickle_file\n",
        "try:\n",
        "    with open(checkpoint_path, 'rb') as file:\n",
        "        clients_classes_df = pickle.load(file)\n",
        "\n",
        "    # Print the loaded DataFrame to verify\n",
        "    print(\"Checkpoint data loaded successfully:\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Checkpoint file not found: {checkpoint_path}\")\n",
        "\n",
        "#data_dir = args.data_dir\n",
        "data_dir = \"/content/drive/MyDrive/MLDL/\"\n",
        "\n",
        "if args.dataset == 'cifar':\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "    train_dataset = datasets.CIFAR100(data_dir, train=True, download=True,\n",
        "                                    transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR100(data_dir, train=False, download=True,\n",
        "                                  transform=transform_test)\n",
        "\n",
        "\n",
        "display(clients_classes_df)\n",
        "if args.dataset == 'cifar':\n",
        "    metrics = pd.DataFrame(columns=['Round', 'Test Accuracy', 'Test Loss', 'Avg Test Accuracy', 'Avg Test Loss', 'Avg Validation Accuracy', 'Avg Validation Loss'])\n",
        "else:\n",
        "    metrics = pd.DataFrame(columns=['Round', 'Test Accuracy', 'Test Loss', 'Avg Test Accuracy', 'Avg Test Loss'])\n",
        "\n",
        "\n",
        "\n",
        "#logger.info(f\"Saved clients classes distribution to {pickle_file}\")\n",
        "\n",
        "if args.n_nodes == args.num_users:\n",
        "    if args.algorithm == 'fedavg':\n",
        "        fedAVG(global_model, clients, criterion, args, temp, metrics, temp, device, test_set)\n",
        "    elif args.algorithm == 'pfedhn':\n",
        "        pFedHN(global_model, clients, criterion, args, metrics, device, test_set)\n",
        "else:\n",
        "    # Assuming df is your DataFrame with a column named 'new_id'\n",
        "    df_sorted = clients_classes_df.sort_values(by='new_id')\n",
        "\n",
        "    # Optionally, reset the index if you want to have the default index after sorting\n",
        "    df_sorted.reset_index(drop=True, inplace=True)\n",
        "    display(df_sorted)\n",
        "\n",
        "\n",
        "    clients = []\n",
        "    for client_id in range(args.num_users):\n",
        "        client = Client(args, df_sorted['client_id'][client_id], train_dataset, test_dataset, df_sorted['train_indices'][client_id], df_sorted['val_indices'][client_id], df_sorted['test_indices'][client_id])\n",
        "\n",
        "        #client = Client(args, clients_classes_df['new_id'][client_id], train_dataset, test_dataset, clients_classes_df['train_indices'][client_id], clients_classes_df['val_indices'][client_id], clients_classes_df['test_indices'][client_id])\n",
        "        clients.append(client)\n",
        "\n",
        "    if args.algorithm == 'fedavg':\n",
        "        fedAVG(global_model, clients, criterion, args, temp, metrics, temp, device, test_set)\n",
        "    elif args.algorithm == 'pfedhn':\n",
        "        # TO DO: add the wandb_logger to the pFedHN\n",
        "        pFedHN(global_model, clients, criterion, args, metrics, device, test_dataset)\n"
      ],
      "metadata": {
        "id": "10DklymOJM10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23862230-f500-4dba-e64c-c5622f696cce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint data loaded successfully:\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    client_id      train        val       test  new_id  \\\n",
              "0           0  {66: 400}  {66: 100}  {66: 100}      52   \n",
              "1           1  {52: 400}  {52: 100}  {52: 100}      57   \n",
              "2           2  {32: 400}  {32: 100}  {32: 100}      33   \n",
              "3           3  {76: 400}  {76: 100}  {76: 100}       3   \n",
              "4           4  {80: 400}  {80: 100}  {80: 100}      15   \n",
              "..        ...        ...        ...        ...     ...   \n",
              "95         95  {54: 400}  {54: 100}  {54: 100}      37   \n",
              "96         96  {51: 400}  {51: 100}  {51: 100}      32   \n",
              "97         97  {92: 400}  {92: 100}  {92: 100}      56   \n",
              "98         98  {63: 400}  {63: 100}  {63: 100}      85   \n",
              "99         99   {0: 400}   {0: 100}   {0: 100}      28   \n",
              "\n",
              "                                        train_indices  \\\n",
              "0   [21798, 9842, 16198, 36577, 32338, 34686, 3768...   \n",
              "1   [38624, 5427, 28266, 41757, 6122, 3113, 9973, ...   \n",
              "2   [23117, 33712, 37542, 31746, 49253, 36127, 552...   \n",
              "3   [35788, 42003, 28492, 39416, 11844, 35000, 498...   \n",
              "4   [19346, 34367, 26146, 41023, 15974, 41326, 383...   \n",
              "..                                                ...   \n",
              "95  [33406, 33249, 40826, 1577, 19205, 29793, 1580...   \n",
              "96  [2882, 40895, 48859, 13023, 35381, 44138, 1830...   \n",
              "97  [8165, 37937, 8080, 27736, 13140, 29705, 11549...   \n",
              "98  [10625, 6989, 20417, 21242, 6953, 42918, 2716,...   \n",
              "99  [16942, 39404, 2500, 33492, 14480, 19265, 9404...   \n",
              "\n",
              "                                          val_indices  \\\n",
              "0   [4812, 13010, 9025, 37095, 39183, 268, 21995, ...   \n",
              "1   [42526, 23261, 13141, 32989, 44141, 41089, 231...   \n",
              "2   [33040, 38597, 30277, 43613, 26454, 40693, 361...   \n",
              "3   [49443, 10426, 11318, 15173, 36490, 35418, 532...   \n",
              "4   [6440, 45794, 28508, 24698, 3026, 1081, 47146,...   \n",
              "..                                                ...   \n",
              "95  [5071, 27780, 1178, 456, 13428, 36758, 26599, ...   \n",
              "96  [16786, 29546, 33842, 39553, 31485, 33216, 982...   \n",
              "97  [25644, 27435, 8485, 11055, 25695, 32326, 4371...   \n",
              "98  [34607, 147, 8330, 2887, 27506, 41920, 41902, ...   \n",
              "99  [41798, 5129, 32159, 2126, 32507, 2264, 35193,...   \n",
              "\n",
              "                                         test_indices  \n",
              "0   [4075, 2619, 6441, 415, 3660, 2546, 6335, 4618...  \n",
              "1   [1803, 7734, 6137, 2283, 7438, 7666, 5265, 114...  \n",
              "2   [3366, 7480, 2465, 9699, 8860, 4954, 689, 6158...  \n",
              "3   [5062, 5355, 5477, 3097, 6744, 1580, 5547, 524...  \n",
              "4   [5089, 1883, 2769, 208, 3051, 9590, 260, 7760,...  \n",
              "..                                                ...  \n",
              "95  [7598, 7997, 1255, 3632, 9430, 8158, 3903, 322...  \n",
              "96  [9847, 7619, 7356, 8788, 5484, 3337, 7425, 350...  \n",
              "97  [4613, 6896, 1504, 2000, 9938, 9254, 6944, 689...  \n",
              "98  [564, 2046, 8403, 7185, 7449, 5055, 6195, 971,...  \n",
              "99  [8211, 4676, 5496, 3775, 9082, 6806, 9221, 466...  \n",
              "\n",
              "[100 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-70505dc2-8e58-454d-9845-eb0a1208480e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>client_id</th>\n",
              "      <th>train</th>\n",
              "      <th>val</th>\n",
              "      <th>test</th>\n",
              "      <th>new_id</th>\n",
              "      <th>train_indices</th>\n",
              "      <th>val_indices</th>\n",
              "      <th>test_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>{66: 400}</td>\n",
              "      <td>{66: 100}</td>\n",
              "      <td>{66: 100}</td>\n",
              "      <td>52</td>\n",
              "      <td>[21798, 9842, 16198, 36577, 32338, 34686, 3768...</td>\n",
              "      <td>[4812, 13010, 9025, 37095, 39183, 268, 21995, ...</td>\n",
              "      <td>[4075, 2619, 6441, 415, 3660, 2546, 6335, 4618...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>{52: 400}</td>\n",
              "      <td>{52: 100}</td>\n",
              "      <td>{52: 100}</td>\n",
              "      <td>57</td>\n",
              "      <td>[38624, 5427, 28266, 41757, 6122, 3113, 9973, ...</td>\n",
              "      <td>[42526, 23261, 13141, 32989, 44141, 41089, 231...</td>\n",
              "      <td>[1803, 7734, 6137, 2283, 7438, 7666, 5265, 114...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>{32: 400}</td>\n",
              "      <td>{32: 100}</td>\n",
              "      <td>{32: 100}</td>\n",
              "      <td>33</td>\n",
              "      <td>[23117, 33712, 37542, 31746, 49253, 36127, 552...</td>\n",
              "      <td>[33040, 38597, 30277, 43613, 26454, 40693, 361...</td>\n",
              "      <td>[3366, 7480, 2465, 9699, 8860, 4954, 689, 6158...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>{76: 400}</td>\n",
              "      <td>{76: 100}</td>\n",
              "      <td>{76: 100}</td>\n",
              "      <td>3</td>\n",
              "      <td>[35788, 42003, 28492, 39416, 11844, 35000, 498...</td>\n",
              "      <td>[49443, 10426, 11318, 15173, 36490, 35418, 532...</td>\n",
              "      <td>[5062, 5355, 5477, 3097, 6744, 1580, 5547, 524...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>{80: 400}</td>\n",
              "      <td>{80: 100}</td>\n",
              "      <td>{80: 100}</td>\n",
              "      <td>15</td>\n",
              "      <td>[19346, 34367, 26146, 41023, 15974, 41326, 383...</td>\n",
              "      <td>[6440, 45794, 28508, 24698, 3026, 1081, 47146,...</td>\n",
              "      <td>[5089, 1883, 2769, 208, 3051, 9590, 260, 7760,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>{54: 400}</td>\n",
              "      <td>{54: 100}</td>\n",
              "      <td>{54: 100}</td>\n",
              "      <td>37</td>\n",
              "      <td>[33406, 33249, 40826, 1577, 19205, 29793, 1580...</td>\n",
              "      <td>[5071, 27780, 1178, 456, 13428, 36758, 26599, ...</td>\n",
              "      <td>[7598, 7997, 1255, 3632, 9430, 8158, 3903, 322...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>{51: 400}</td>\n",
              "      <td>{51: 100}</td>\n",
              "      <td>{51: 100}</td>\n",
              "      <td>32</td>\n",
              "      <td>[2882, 40895, 48859, 13023, 35381, 44138, 1830...</td>\n",
              "      <td>[16786, 29546, 33842, 39553, 31485, 33216, 982...</td>\n",
              "      <td>[9847, 7619, 7356, 8788, 5484, 3337, 7425, 350...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>{92: 400}</td>\n",
              "      <td>{92: 100}</td>\n",
              "      <td>{92: 100}</td>\n",
              "      <td>56</td>\n",
              "      <td>[8165, 37937, 8080, 27736, 13140, 29705, 11549...</td>\n",
              "      <td>[25644, 27435, 8485, 11055, 25695, 32326, 4371...</td>\n",
              "      <td>[4613, 6896, 1504, 2000, 9938, 9254, 6944, 689...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>{63: 400}</td>\n",
              "      <td>{63: 100}</td>\n",
              "      <td>{63: 100}</td>\n",
              "      <td>85</td>\n",
              "      <td>[10625, 6989, 20417, 21242, 6953, 42918, 2716,...</td>\n",
              "      <td>[34607, 147, 8330, 2887, 27506, 41920, 41902, ...</td>\n",
              "      <td>[564, 2046, 8403, 7185, 7449, 5055, 6195, 971,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>{0: 400}</td>\n",
              "      <td>{0: 100}</td>\n",
              "      <td>{0: 100}</td>\n",
              "      <td>28</td>\n",
              "      <td>[16942, 39404, 2500, 33492, 14480, 19265, 9404...</td>\n",
              "      <td>[41798, 5129, 32159, 2126, 32507, 2264, 35193,...</td>\n",
              "      <td>[8211, 4676, 5496, 3775, 9082, 6806, 9221, 466...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70505dc2-8e58-454d-9845-eb0a1208480e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-70505dc2-8e58-454d-9845-eb0a1208480e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-70505dc2-8e58-454d-9845-eb0a1208480e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e96759f5-f37e-4dfa-9add-160108866bfc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e96759f5-f37e-4dfa-9add-160108866bfc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e96759f5-f37e-4dfa-9add-160108866bfc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a5b4556c-c6bd-45dc-aef6-33639c50c2e0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('clients_classes_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a5b4556c-c6bd-45dc-aef6-33639c50c2e0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('clients_classes_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "clients_classes_df",
              "summary": "{\n  \"name\": \"clients_classes_df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"client_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          62,\n          13,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    client_id      train        val       test  new_id  \\\n",
              "0          11  {61: 400}  {61: 100}  {61: 100}       0   \n",
              "1          81   {4: 400}   {4: 100}   {4: 100}       1   \n",
              "2          77   {2: 400}   {2: 100}   {2: 100}       2   \n",
              "3           3  {76: 400}  {76: 100}  {76: 100}       3   \n",
              "4          65  {53: 400}  {53: 100}  {53: 100}       4   \n",
              "..        ...        ...        ...        ...     ...   \n",
              "95         60  {29: 400}  {29: 100}  {29: 100}      95   \n",
              "96         12  {98: 400}  {98: 100}  {98: 100}      96   \n",
              "97         50   {7: 400}   {7: 100}   {7: 100}      97   \n",
              "98         71  {12: 400}  {12: 100}  {12: 100}      98   \n",
              "99         58   {6: 400}   {6: 100}   {6: 100}      99   \n",
              "\n",
              "                                        train_indices  \\\n",
              "0   [43031, 42082, 30244, 3309, 18975, 38580, 4852...   \n",
              "1   [30665, 38204, 27342, 39748, 35882, 19181, 135...   \n",
              "2   [9964, 580, 382, 16332, 27661, 12363, 47261, 3...   \n",
              "3   [35788, 42003, 28492, 39416, 11844, 35000, 498...   \n",
              "4   [49422, 26610, 18645, 28859, 44041, 20464, 302...   \n",
              "..                                                ...   \n",
              "95  [31711, 30207, 14744, 31805, 24054, 27887, 158...   \n",
              "96  [45304, 740, 29937, 42097, 47458, 5459, 46462,...   \n",
              "97  [31534, 25681, 5516, 38429, 3810, 7173, 30265,...   \n",
              "98  [38619, 23655, 14684, 19663, 25921, 6031, 4694...   \n",
              "99  [23387, 2017, 35141, 31356, 22066, 1320, 22877...   \n",
              "\n",
              "                                          val_indices  \\\n",
              "0   [38599, 25239, 37441, 29374, 49725, 20029, 492...   \n",
              "1   [46937, 31311, 32117, 954, 10099, 10243, 21109...   \n",
              "2   [46974, 4031, 31300, 13524, 47390, 36345, 8681...   \n",
              "3   [49443, 10426, 11318, 15173, 36490, 35418, 532...   \n",
              "4   [11550, 22135, 49432, 8198, 46248, 13245, 1948...   \n",
              "..                                                ...   \n",
              "95  [36208, 9684, 28788, 29096, 37652, 10355, 3919...   \n",
              "96  [10317, 22963, 11504, 20033, 25848, 34236, 388...   \n",
              "97  [39476, 43315, 23862, 26164, 32032, 14553, 214...   \n",
              "98  [10034, 48415, 6835, 20126, 17653, 35516, 2545...   \n",
              "99  [42085, 8221, 2518, 11745, 33819, 27682, 34185...   \n",
              "\n",
              "                                         test_indices  \n",
              "0   [4562, 9064, 5318, 1492, 8195, 7663, 1625, 197...  \n",
              "1   [8400, 4951, 2720, 4749, 3380, 1379, 9158, 885...  \n",
              "2   [2777, 3397, 2804, 4071, 6301, 4056, 573, 1047...  \n",
              "3   [5062, 5355, 5477, 3097, 6744, 1580, 5547, 524...  \n",
              "4   [1457, 9449, 245, 793, 9956, 2467, 1706, 8903,...  \n",
              "..                                                ...  \n",
              "95  [5598, 4632, 8888, 3459, 8984, 9772, 386, 2050...  \n",
              "96  [3879, 2020, 1468, 5061, 2498, 7114, 8601, 892...  \n",
              "97  [2201, 64, 7684, 8294, 9652, 5518, 3103, 198, ...  \n",
              "98  [6905, 5423, 324, 5814, 7116, 1194, 6861, 1649...  \n",
              "99  [7273, 7839, 535, 2461, 816, 2517, 3995, 7091,...  \n",
              "\n",
              "[100 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cae1550c-c5ea-40b5-86b9-f67b7a549a3c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>client_id</th>\n",
              "      <th>train</th>\n",
              "      <th>val</th>\n",
              "      <th>test</th>\n",
              "      <th>new_id</th>\n",
              "      <th>train_indices</th>\n",
              "      <th>val_indices</th>\n",
              "      <th>test_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>{61: 400}</td>\n",
              "      <td>{61: 100}</td>\n",
              "      <td>{61: 100}</td>\n",
              "      <td>0</td>\n",
              "      <td>[43031, 42082, 30244, 3309, 18975, 38580, 4852...</td>\n",
              "      <td>[38599, 25239, 37441, 29374, 49725, 20029, 492...</td>\n",
              "      <td>[4562, 9064, 5318, 1492, 8195, 7663, 1625, 197...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>81</td>\n",
              "      <td>{4: 400}</td>\n",
              "      <td>{4: 100}</td>\n",
              "      <td>{4: 100}</td>\n",
              "      <td>1</td>\n",
              "      <td>[30665, 38204, 27342, 39748, 35882, 19181, 135...</td>\n",
              "      <td>[46937, 31311, 32117, 954, 10099, 10243, 21109...</td>\n",
              "      <td>[8400, 4951, 2720, 4749, 3380, 1379, 9158, 885...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>77</td>\n",
              "      <td>{2: 400}</td>\n",
              "      <td>{2: 100}</td>\n",
              "      <td>{2: 100}</td>\n",
              "      <td>2</td>\n",
              "      <td>[9964, 580, 382, 16332, 27661, 12363, 47261, 3...</td>\n",
              "      <td>[46974, 4031, 31300, 13524, 47390, 36345, 8681...</td>\n",
              "      <td>[2777, 3397, 2804, 4071, 6301, 4056, 573, 1047...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>{76: 400}</td>\n",
              "      <td>{76: 100}</td>\n",
              "      <td>{76: 100}</td>\n",
              "      <td>3</td>\n",
              "      <td>[35788, 42003, 28492, 39416, 11844, 35000, 498...</td>\n",
              "      <td>[49443, 10426, 11318, 15173, 36490, 35418, 532...</td>\n",
              "      <td>[5062, 5355, 5477, 3097, 6744, 1580, 5547, 524...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>65</td>\n",
              "      <td>{53: 400}</td>\n",
              "      <td>{53: 100}</td>\n",
              "      <td>{53: 100}</td>\n",
              "      <td>4</td>\n",
              "      <td>[49422, 26610, 18645, 28859, 44041, 20464, 302...</td>\n",
              "      <td>[11550, 22135, 49432, 8198, 46248, 13245, 1948...</td>\n",
              "      <td>[1457, 9449, 245, 793, 9956, 2467, 1706, 8903,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>60</td>\n",
              "      <td>{29: 400}</td>\n",
              "      <td>{29: 100}</td>\n",
              "      <td>{29: 100}</td>\n",
              "      <td>95</td>\n",
              "      <td>[31711, 30207, 14744, 31805, 24054, 27887, 158...</td>\n",
              "      <td>[36208, 9684, 28788, 29096, 37652, 10355, 3919...</td>\n",
              "      <td>[5598, 4632, 8888, 3459, 8984, 9772, 386, 2050...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>12</td>\n",
              "      <td>{98: 400}</td>\n",
              "      <td>{98: 100}</td>\n",
              "      <td>{98: 100}</td>\n",
              "      <td>96</td>\n",
              "      <td>[45304, 740, 29937, 42097, 47458, 5459, 46462,...</td>\n",
              "      <td>[10317, 22963, 11504, 20033, 25848, 34236, 388...</td>\n",
              "      <td>[3879, 2020, 1468, 5061, 2498, 7114, 8601, 892...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>50</td>\n",
              "      <td>{7: 400}</td>\n",
              "      <td>{7: 100}</td>\n",
              "      <td>{7: 100}</td>\n",
              "      <td>97</td>\n",
              "      <td>[31534, 25681, 5516, 38429, 3810, 7173, 30265,...</td>\n",
              "      <td>[39476, 43315, 23862, 26164, 32032, 14553, 214...</td>\n",
              "      <td>[2201, 64, 7684, 8294, 9652, 5518, 3103, 198, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>71</td>\n",
              "      <td>{12: 400}</td>\n",
              "      <td>{12: 100}</td>\n",
              "      <td>{12: 100}</td>\n",
              "      <td>98</td>\n",
              "      <td>[38619, 23655, 14684, 19663, 25921, 6031, 4694...</td>\n",
              "      <td>[10034, 48415, 6835, 20126, 17653, 35516, 2545...</td>\n",
              "      <td>[6905, 5423, 324, 5814, 7116, 1194, 6861, 1649...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>58</td>\n",
              "      <td>{6: 400}</td>\n",
              "      <td>{6: 100}</td>\n",
              "      <td>{6: 100}</td>\n",
              "      <td>99</td>\n",
              "      <td>[23387, 2017, 35141, 31356, 22066, 1320, 22877...</td>\n",
              "      <td>[42085, 8221, 2518, 11745, 33819, 27682, 34185...</td>\n",
              "      <td>[7273, 7839, 535, 2461, 816, 2517, 3995, 7091,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cae1550c-c5ea-40b5-86b9-f67b7a549a3c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cae1550c-c5ea-40b5-86b9-f67b7a549a3c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cae1550c-c5ea-40b5-86b9-f67b7a549a3c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f9eaf9b8-f612-42ea-8e3b-efe3882505e5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9eaf9b8-f612-42ea-8e3b-efe3882505e5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f9eaf9b8-f612-42ea-8e3b-efe3882505e5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c5e31354-5bc2-40f3-b9b2-85981627636b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_sorted')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c5e31354-5bc2-40f3-b9b2-85981627636b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_sorted');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_sorted",
              "summary": "{\n  \"name\": \"df_sorted\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"client_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          49,\n          30,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pFedHN' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a4eb854bfc8b>\u001b[0m in \u001b[0;36m<cell line: 103>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pfedhn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# TO DO: add the wandb_logger to the pFedHN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mpFedHN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pFedHN' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generalization Hypernetwork"
      ],
      "metadata": {
        "id": "ygt5kOEf7T5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def pFedHN_Gen(global_model, clients, criterion, args, logger, metrics, wandb_logger, device, test_set):\n",
        "def pFedHN_Gen(global_model, clients, criterion, args, metrics, device, test_set):\n",
        "\n",
        "    nodes = clients\n",
        "\n",
        "    clients_distribs = {client.client_id: 0 for client in clients}\n",
        "    embed_dim = args.embed_dim\n",
        "    num_nodes = args.n_nodes\n",
        "    num_users = args.num_users\n",
        "\n",
        "    # Print the ID and indices of the first 5 clients\n",
        "    for i in range(min(5, len(clients))):  # To ensure we only print if there are at least 5 clients\n",
        "        print(f\"Client ID: {clients[i].client_id}\")\n",
        "        print(f\"Train indices: {clients[i].train_indices[:10]}\")  # Print only the first 10 indices for brevity\n",
        "        print(f\"Val indices: {clients[i].val_indices[:10]}\")  # Print only the first 10 indices\n",
        "        print(f\"Test indices: {clients[i].test_indices[:10]}\")  # Print only the first 10 indices\n",
        "        print('-' * 50)\n",
        "\n",
        "    if embed_dim == -1:\n",
        "        embed_dim = int(1 + num_nodes / 4)\n",
        "\n",
        "    last_epoch = args.last_epoch\n",
        "    #last_epoch = 1900\n",
        "    if args.participation:\n",
        "        checkpoint_pattern = f\"{args.checkpoint_path}/checkpoint_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}_epoch_{args.last_epoch}.pth.tar\"\n",
        "    else:\n",
        "        checkpoint_pattern = f\"{args.checkpoint_path}/checkpoint_{args.algorithm}_{args.iid}_{args.participation}_{args.gamma}_{args.Nc}_{args.local_ep}_epoch_{args.last_epoch}.pth.tar\"\n",
        "    checkpoint_files = sorted(glob.glob(checkpoint_pattern))\n",
        "    print(checkpoint_pattern)\n",
        "    print(\"len:\",len(checkpoint_files))\n",
        "\n",
        "    if len(checkpoint_files):\n",
        "        latest_checkpoint = checkpoint_files[-1]\n",
        "        checkpoint = load_checkpoint(latest_checkpoint)\n",
        "\n",
        "        if checkpoint:\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            last_user_input = checkpoint['user_input']\n",
        "            test_acc = checkpoint['test_accuracy']\n",
        "            test_avg_loss = checkpoint['test_avg_loss']\n",
        "            test_avg_acc = checkpoint['test_avg_acc']\n",
        "            val_acc = checkpoint['val_accuracy']\n",
        "            val_avg_loss = checkpoint['val_avg_loss']\n",
        "            val_avg_acc = checkpoint['val_avg_acc']\n",
        "            # Print the status of the last checkpoint\n",
        "            participation_status = 'uniform' if last_user_input[1] == 1 else 'skewed'\n",
        "            user_input_string = f\"IID: {last_user_input[0]}, Participation: {participation_status}, Nc: {last_user_input[2]}, J: {last_user_input[3]}\"\n",
        "\n",
        "            #logger.info(f\"\\nA saving checkpoint with these parameters exists:\\n\"\n",
        "            print(f\"\\nA saving checkpoint with these parameters exists:\\n\"\n",
        "                f\"Last checkpoint details:\\n\"\n",
        "                f\"Epoch reached: {start_epoch}\\n\"\n",
        "                f\"User input variables: {user_input_string}\\n\"\n",
        "                f'Test Accuracy: {100*test_acc}%\\n'\n",
        "                f'Test Avg Loss: {test_avg_loss}\\n'\n",
        "                f'Test Avg Acc: {100*test_avg_acc}%\\n'\n",
        "                f'Val Accuracy: {100*val_acc}%\\n'\n",
        "                f'Val Avg Loss {val_avg_loss}\\n'\n",
        "                f'Val Avg Acc {100*val_avg_acc}%\\n')\n",
        "\n",
        "\n",
        "            # print(\"\\nitems:\\n\",checkpoint['hn_state_dict'].items())\n",
        "            if args.checkpoint_resume == 1:\n",
        "\n",
        "                global_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                net = global_model\n",
        "\n",
        "                hnet = CNNHyper(num_users, embed_dim).to(device)\n",
        "                hnet_old = CNNHyper_old(num_nodes, embed_dim).to(device)\n",
        "\n",
        "                # Load all other parameters except the embedding layer\n",
        "                hnet_state_dict = hnet.state_dict()\n",
        "                hnet_old.load_state_dict(checkpoint['hn_state_dict'])\n",
        "                checkpoint_hnet_state_dict = checkpoint['hn_state_dict']\n",
        "\n",
        "                # Copy weights for all layers except the embeddings\n",
        "                for name, param in checkpoint_hnet_state_dict.items():\n",
        "                    if 'embeddings' not in name:\n",
        "                        hnet_state_dict[name] = param\n",
        "\n",
        "                # Manually load the embeddings, adjusting for the new size\n",
        "                embedding_key = 'embeddings.weight'\n",
        "                if embedding_key in checkpoint_hnet_state_dict:\n",
        "                    old_embeddings = checkpoint_hnet_state_dict[embedding_key]\n",
        "                    new_embeddings = hnet_state_dict[embedding_key]\n",
        "\n",
        "                    # Copy the first part of the old embeddings to the new embeddings\n",
        "                    new_embeddings[:old_embeddings.size(0)] = old_embeddings\n",
        "                    if args.average:\n",
        "                        # Compute the average vector of the old embeddings\n",
        "                        average_vector = old_embeddings.mean(dim=0)\n",
        "\n",
        "                        # Fill the rest of the new_embeddings with the average vector\n",
        "                        new_embeddings[old_embeddings.size(0):] = average_vector\n",
        "                        display(new_embeddings)\n",
        "                    hnet_state_dict[embedding_key] = new_embeddings\n",
        "\n",
        "                hnet.load_state_dict(hnet_state_dict)\n",
        "\n",
        "                #logger.info(f\"Starting the vector embedding tuning\")\n",
        "                \"\"\"\n",
        "                if args.update:\n",
        "                    for name, param in hnet.named_parameters():\n",
        "                        if 'embed' not in name:\n",
        "                            param.requires_grad = False\n",
        "                        else:\n",
        "                            param_to_compute_grad = param\"\"\"\n",
        "\n",
        "    ##################\n",
        "    # init optimizer #\n",
        "    ##################\n",
        "    lr = args.lr\n",
        "    embed_lr = args.embed_lr\n",
        "    wd = args.wd\n",
        "\n",
        "    embed_lr = embed_lr if embed_lr is not None else lr\n",
        "    optimizers = {\n",
        "        'sgd': torch.optim.SGD(\n",
        "            [\n",
        "                {'params': [p for n, p in hnet.named_parameters() if 'embed' not in n]},\n",
        "                {'params': [p for n, p in hnet.named_parameters() if 'embed' in n], 'lr': embed_lr}\n",
        "            ], lr=lr, weight_decay=wd\n",
        "        )\n",
        "    }\n",
        "    optimizer = optimizers[args.optimizer]\n",
        "\n",
        "    ################\n",
        "    # init metrics #\n",
        "    ################\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    dirichlet_probs = np.random.dirichlet([args.gamma] * num_users)\n",
        "\n",
        "    old_test_accuracy = checkpoint['test_accuracy']\n",
        "    old_val_accuracy = checkpoint['val_accuracy']\n",
        "\n",
        "    step_results_old, avg_loss_old, avg_acc_old, acc_old = eval_pfedhn(nodes, num_nodes, hnet_old, net, criterion, device, loader_type=\"test\")\n",
        "    print(\"\\n the old model accuracy on old client is\", acc_old)\n",
        "    step_results, test_avg_loss_new, test_avg_acc_new, test_loss, test_acc = eval_pfedhn_gen(nodes, num_nodes, num_users , hnet, net, criterion, device, loader_type=\"test\")\n",
        "    newmodel_avg_acc_old = np.mean([step_results[node_id]['accuracy'] for node_id in range(0, num_nodes)])\n",
        "    print(\"\\nthe new model test accuracy on old client is: \", newmodel_avg_acc_old)\n",
        "\n",
        "    #step_iter = trange(args.epochs - last_epoch + args.extra)\n",
        "    step_iter = trange(args.step_iter)\n",
        "    for step in step_iter:\n",
        "        hnet.train()\n",
        "\n",
        "        # Compute the weights for old clients and new clients based on the bias factor\n",
        "        num_old_clients = num_nodes\n",
        "        num_new_clients = num_users - num_nodes\n",
        "        bias_factor = args.bias\n",
        "\n",
        "        if bias_factor <= 1:\n",
        "            weight_old_clients = 1.0 - bias_factor  # decreases from 1 to 0\n",
        "            weight_new_clients = bias_factor        # increases from 0 to 1\n",
        "\n",
        "        # Create the weights array\n",
        "        weights = np.array([weight_old_clients] * num_old_clients + [weight_new_clients] * num_new_clients)\n",
        "\n",
        "\n",
        "        # Normalize the weights to create a probability distribution\n",
        "        prob_distribution = weights / weights.sum()\n",
        "\n",
        "        # Randomly select a client based on the probability distribution\n",
        "        node_id = np.random.choice(range(num_users), p=prob_distribution)\n",
        "\n",
        "        #clients_distribs[node_id] = 1\n",
        "        # produce & load local network weights\n",
        "        weights = hnet(torch.tensor([node_id], dtype=torch.long).to(device))\n",
        "        net.load_state_dict(weights)\n",
        "\n",
        "        # init inner optimizer\n",
        "        inner_optim = torch.optim.SGD(\n",
        "            net.parameters(), lr=args.lr, weight_decay=args.inner_wd\n",
        "        )\n",
        "\n",
        "        # storing theta_i for later calculating delta theta\n",
        "        inner_state = OrderedDict({k: tensor.data for k, tensor in weights.items()})\n",
        "\n",
        "        # inner updates -> obtaining theta_tilda\n",
        "        inner_steps = args.local_ep * 10\n",
        "        for i in range(inner_steps):\n",
        "            net.train()\n",
        "            inner_optim.zero_grad()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch = next(iter(nodes[node_id].train_dataloader))\n",
        "            img, label = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            pred = net(img)\n",
        "\n",
        "            loss = criterion(pred, label)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), 50)\n",
        "\n",
        "            inner_optim.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        final_state = net.state_dict()\n",
        "\n",
        "        # calculating delta theta\n",
        "        delta_theta = OrderedDict({k: inner_state[k] - final_state[k] for k in weights.keys()})\n",
        "\n",
        "        if args.update:\n",
        "            \"\"\"hnet_grad = torch.autograd.grad(\n",
        "                outputs=list(weights.values())[0],\n",
        "                inputs=param_to_compute_grad,\n",
        "                grad_outputs=list(delta_theta.values())[0]\n",
        "            )\n",
        "\n",
        "\n",
        "            # update hnet weights\n",
        "            for p, g in zip(hnet.parameters(), hnet_grad):\n",
        "              if p.requires_grad:  # Only assign gradients if requires_grad is True\n",
        "                  p.grad = g\"\"\"\n",
        "            # calculating phi gradient\n",
        "            hnet_grads = torch.autograd.grad(\n",
        "                list(weights.values()), hnet.parameters(), grad_outputs=list(delta_theta.values())\n",
        "            )\n",
        "\n",
        "\n",
        "            next(hnet.parameters()).grad = hnet_grads[0]\n",
        "\n",
        "        else:\n",
        "            # calculating phi gradient\n",
        "            hnet_grads = torch.autograd.grad(\n",
        "                list(weights.values()), hnet.parameters(), grad_outputs=list(delta_theta.values())\n",
        "            )\n",
        "\n",
        "            # update hnet weights\n",
        "            for p, g in zip(hnet.parameters(), hnet_grads):\n",
        "                # Only assign gradients if requires_grad is True\n",
        "                  p.grad = g\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(hnet.parameters(), 50)\n",
        "        optimizer.step()\n",
        "\n",
        "        # logger.info(f\"\\n\\nStep: {step+1}, Node ID: {node_id}, Loss: {prvs_loss:.4f},  Acc: {prvs_acc:.4f}\")\n",
        "        if (step +1) % args.print_every == 0:\n",
        "\n",
        "            filename = f\"{args.checkpoint_path}/checkpoint_{args.algorithm}_{args.bias}_gen_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}_epoch_{args.last_epoch+step+1}.pth.tar\"\n",
        "\n",
        "            last_eval = step\n",
        "            step_results, test_avg_loss_new, test_avg_acc_new, test_loss, test_acc = eval_pfedhn_gen(nodes, num_nodes, num_users , hnet, net, criterion, device, loader_type=\"test\")\n",
        "            print(f\"\\nStep: {step+1}, New Clients Test Loss: {test_avg_loss_new:.4f},  New Clients Test Acc: {test_avg_acc_new:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "            # logger.info(f\"\\nStep: {step+1}, AVG Loss: {avg_loss:.4f},  AVG Acc: {avg_acc:.4f}\")\n",
        "            \"\"\"wandb_logger.log({\n",
        "                'Step': step + 1,\n",
        "                'Test Avg Loss': avg_loss,\n",
        "                'Test Avg Acc': avg_acc,\n",
        "            })\"\"\"\n",
        "            results['test_avg_loss_new'].append(float(f\"{test_avg_loss_new:.4f}\"))\n",
        "            results['test_avg_acc_new'].append(float(f\"{test_avg_acc_new:.4f}\"))\n",
        "            results['test_loss'].append(float(f\"{test_loss:.4f}\"))\n",
        "            results['test_acc'].append(float(f\"{test_acc:.4f}\"))\n",
        "\n",
        "            _, val_avg_loss_new, val_avg_acc_new, val_loss, val_acc  = eval_pfedhn_gen(nodes, num_nodes, num_users, hnet, net, criterion, device, loader_type=\"val\")\n",
        "            results['val_avg_loss_new'].append(float(f\"{val_avg_loss_new:.4f}\"))\n",
        "            results['val_avg_acc_new'].append(float(f\"{val_avg_acc_new:.4f}\"))\n",
        "            results['val_loss'].append(float(f\"{val_loss:.4f}\"))\n",
        "            results['val_acc'].append(float(f\"{val_acc:.4f}\"))\n",
        "\n",
        "            print(\"\\ncheck acc:\", old_test_accuracy)\n",
        "            print(\"\\nnew client acc:\", test_avg_acc_new)\n",
        "            flag = 0\n",
        "            if (old_test_accuracy - results['test_avg_acc_new'][-1]) < 0:\n",
        "\n",
        "                flag = 1\n",
        "\n",
        "            \"\"\"wandb_logger.log({\n",
        "                'Test Loss': loss,\n",
        "                'Test Accuracy': acc * 100,\n",
        "                'Round': step + 1\n",
        "            })\"\"\"\n",
        "            for key in results:\n",
        "                print(f\"{key}: {results[key][-1]}\")\n",
        "\n",
        "                #logger.info(f\"{key}: {results[key][-1]}\")\n",
        "                #wandb_logger.log({key: results[key][-1]})\n",
        "\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': step + 1,\n",
        "                'epoch_start': last_epoch,\n",
        "                'model_state_dict': global_model.state_dict(),\n",
        "                'hn_state_dict': hnet.state_dict(),\n",
        "                'user_input': (args.iid, args.participation, args.Nc, args.local_ep),\n",
        "                'test_avg_loss_new': results['test_avg_loss_new'][-1],\n",
        "                'test_avg_acc_new': results['test_avg_acc_new'][-1],\n",
        "                'test_loss': results['test_loss'][-1],\n",
        "                'test_acc': results['test_acc'][-1],\n",
        "                'val_avg_loss_new': results['val_avg_loss_new'][-1],\n",
        "                'val_avg_acc_new': results['val_avg_acc_new'][-1],\n",
        "                'val_loss': results['val_loss'][-1],\n",
        "                'val_acc': results['val_acc'][-1],\n",
        "                'old test accuracy': old_test_accuracy,\n",
        "                'old val accuracy': old_val_accuracy\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=filename)\n",
        "\n",
        "            # Remove the previous checkpoint unless it's a multiple of the backup parameter\n",
        "            prev_epoch = step + 1 - args.print_every\n",
        "            if (step + 1) > args.print_every and prev_epoch != 100:\n",
        "                if (step + 1 -10) % args.backup != 0:\n",
        "                    prev_filename = f\"{args.checkpoint_path}/checkpoint_{args.algorithm}_{args.bias}_gen_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}_epoch_{1900+prev_epoch}.pth.tar\"\n",
        "                    if os.path.exists(prev_filename):\n",
        "                        os.remove(prev_filename)\n",
        "            # metrics = pd.DataFrame(columns=['Round', 'Test Accuracy', 'Test Loss', 'Avg Test Accuracy', 'Avg Test Loss', 'Avg Validation Accuracy', 'Avg Validation Loss'])\n",
        "\n",
        "            metrics.loc[len(metrics)] = [\n",
        "                step + 1,\n",
        "                last_epoch,\n",
        "                results['test_avg_loss_new'][-1],\n",
        "                results['test_avg_acc_new'][-1],\n",
        "                results['test_loss'][-1],\n",
        "                results['test_acc'][-1],\n",
        "                results['val_avg_loss_new'][-1],\n",
        "                results['val_avg_acc_new'][-1],\n",
        "                results['val_loss'][-1],\n",
        "                results['val_acc'][-1],\n",
        "                old_test_accuracy,\n",
        "                old_val_accuracy\n",
        "            ]\n",
        "\n",
        "            if flag == 1:\n",
        "                print(\"convergence found\")\n",
        "                print(\"Global Test Acc Diff:\", old_test_accuracy - test_acc)\n",
        "                print(\"Global Val Acc Diff:\", old_val_accuracy - val_acc)\n",
        "                print(\"New Clients Test Acc:\", results['test_avg_acc_new'][-1])\n",
        "                print(\"New Clients Val Acc:\", results['val_avg_acc_new'][-1])\n",
        "                break\n",
        "\n",
        "    \"\"\"if step != last_eval:\n",
        "        _, val_avg_loss, val_avg_acc, val_acc  = eval_pfedhn_gen(nodes, num_nodes, num_users, hnet, net, criterion, device, loader_type=\"val\")\n",
        "        results['val_avg_loss_new'].append(float(f\"{val_avg_loss_new:.4f}\"))\n",
        "        results['val_avg_acc_new'].append(float(f\"{val_avg_acc_new:.4f}\"))\n",
        "        results['val_loss'].append(float(f\"{val_loss:.4f}\"))\n",
        "        results['val_acc'].append(float(f\"{val_acc:.4f}\"))\n",
        "\n",
        "        step_results, test_avg_loss, test_avg_acc, test_acc = eval_pfedhn_gen(nodes, num_users, num_nodes, hnet, net, criterion, device, loader_type=\"test\")\n",
        "        print(f\"\\nStep: {step+1}, New Clients Test Loss: {test_avg_loss_new:.4f},  New Clients Test Acc: {test_avg_acc_new:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "        results['test_avg_loss_new'].append(float(f\"{test_avg_loss_new:.4f}\"))\n",
        "        results['test_avg_acc_new'].append(float(f\"{test_avg_acc_new:.4f}\"))\n",
        "        results['test_loss'].append(float(f\"{test_loss:.4f}\"))\n",
        "        results['test_acc'].append(float(f\"{test_acc:.4f}\"))\"\"\"\n",
        "    \"\"\"# Plot the frequency of client selection\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Normalize the selection counts\n",
        "    normalized_counts = [count / sum(clients_distribs.values()) for count in clients_distribs.values()]\n",
        "\n",
        "    # Create the bar plot\n",
        "    plt.bar(clients_distribs.keys(), normalized_counts)\n",
        "    plt.xlabel('Client ID')\n",
        "    plt.ylabel('Relative frequency')\n",
        "    if args.participation:\n",
        "        plt.title(f'Clients distribution (random selection)')\n",
        "\n",
        "    else:\n",
        "        plt.title(f'Clients distribution (gamma={args.gamma})')\"\"\"\n",
        "\n",
        "    # Save the plot as a PDF file\n",
        "    if args.participation:\n",
        "        pickle_file = f\"{args.metrics_dir}/metrics_{args.algorithm}_{args.bias}_gen_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}.pkl\"\n",
        "        #plot_location = f'{args.metrics_dir}/client_selection_frequency_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}.pdf'\n",
        "    else:\n",
        "        pickle_file = f\"{args.metrics_dir}/metrics_{args.algorithm}_{args.bias}_gen_{args.iid}_{args.participation}_{args.gamma}_{args.Nc}_{args.local_ep}.pkl\"\n",
        "        #plot_location = f'{args.metrics_dir}/client_selection_frequency_{args.algorithm}_{args.iid}_{args.participation}_{args.gamma}_{args.Nc}_{args.local_ep}.pdf'\n",
        "    #plt.savefig(plot_location)\n",
        "\n",
        "    # Optionally, clear the figure to free up memory\n",
        "    #plt.clf()\n",
        "\n",
        "    metrics.to_pickle(pickle_file)\n",
        "    #logger.info(f\"Metrics saved at {pickle_file}\")\n",
        "    #logger.info(f\"Plots saved at {plot_location}\")\n",
        "    #logger.info(\"Training Done!\")"
      ],
      "metadata": {
        "id": "uw6fmP0KOpo-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generalization Main"
      ],
      "metadata": {
        "id": "OeaQsBlfgMHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from tqdm import trange\n",
        "import pickle\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "args = args_parser()\n",
        "args.iid = 0\n",
        "args.participation = 1\n",
        "args.algorithm = \"pfedhn\"\n",
        "args.Nc = 1\n",
        "args.local_ep = 4\n",
        "args.checkpoint_resume = 1\n",
        "args.checkpoint_path = \"/content/drive/MyDrive/MLDL/Cifar-100/Checkpoints\"\n",
        "args.metrics_dir =  '/content/drive/MyDrive/MLDL/cifar/metrics'\n",
        "args.bias = 1\n",
        "args.step_iter = 100\n",
        "args.extra = 0\n",
        "args.average = 0\n",
        "args.update = 0\n",
        "args.last_epoch = 500\n",
        "\n",
        "if args.gpu:\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "device = 'cuda' if args.gpu else 'cpu'\n",
        "device = 'cpu'\n",
        "#train_set, test_set, clients = get_dataset(args)\n",
        "\n",
        "prev_epoch = 240\n",
        "if args.participation:\n",
        "    pickle_file = f\"{args.metrics_dir}/clients_classes_dist_{args.algorithm}_{args.iid}_{args.participation}_{args.Nc}_{args.local_ep}.pkl\"\n",
        "else:\n",
        "    pickle_file = f\"{args.metrics_dir}/clients_classes_dist_{args.algorithm}_{args.iid}_{args.participation}_{args.gamma}_{args.Nc}_{args.local_ep}.pkl\"\n",
        "# Load the checkpoint\n",
        "checkpoint_path = pickle_file\n",
        "try:\n",
        "    with open(checkpoint_path, 'rb') as file:\n",
        "        clients_classes_df = pickle.load(file)\n",
        "\n",
        "    # Print the loaded DataFrame to verify\n",
        "    print(\"Checkpoint data loaded successfully:\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Checkpoint file not found: {checkpoint_path}\")\n",
        "\n",
        "#data_dir = args.data_dir\n",
        "data_dir = \"/content/drive/MyDrive/MLDL/\"\n",
        "\n",
        "if args.dataset == 'cifar':\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "    train_dataset = datasets.CIFAR100(data_dir, train=True, download=True,\n",
        "                                    transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR100(data_dir, train=False, download=True,\n",
        "                                  transform=transform_test)\n",
        "display(clients_classes_df)\n",
        "# Assuming df is your DataFrame with a column named 'new_id'\n",
        "df_sorted = clients_classes_df.sort_values(by='new_id')\n",
        "\n",
        "# Optionally, reset the index if you want to have the default index after sorting\n",
        "df_sorted.reset_index(drop=True, inplace=True)\n",
        "display(df_sorted)\n",
        "\n",
        "\n",
        "clients = []\n",
        "for client_id in range(args.num_users):\n",
        "    client = Client(args, df_sorted['client_id'][client_id], train_dataset, test_dataset, df_sorted['train_indices'][client_id], df_sorted['val_indices'][client_id], df_sorted['test_indices'][client_id])\n",
        "\n",
        "    #client = Client(args, clients_classes_df['new_id'][client_id], train_dataset, test_dataset, clients_classes_df['train_indices'][client_id], clients_classes_df['val_indices'][client_id], clients_classes_df['test_indices'][client_id])\n",
        "    clients.append(client)\n",
        "\n",
        "# Print the loaded data to verify\n",
        "\"\"\"for client in clients:\n",
        "    client.print_class_distribution()\"\"\"\n",
        "\n",
        "# Ahmad this is the part that will go in federated.py,\n",
        "# you will add checks that based on args.algorithm will select which algorithm we want to run\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "global_model = CIFARLeNet().to(device)\n",
        "\n",
        "# cleints\n",
        "\n",
        "# this will be added in the algorithm.py\n",
        "\"\"\"if args.generalization\n",
        "    random select 90 clients form clients and create the two clients vectors\n",
        "     \"\"\"\n",
        "metrics = pd.DataFrame(columns=[\n",
        "                'Round',\n",
        "                'Start',\n",
        "                'New Clients Test Loss',\n",
        "                'New Clients Test Accuracy',\n",
        "                'Test Loss',\n",
        "                'Test Accuracy',\n",
        "                'New Clients Val Loss',\n",
        "                'New Clients Val Accuracy',\n",
        "                'Val Loss',\n",
        "                'Val Accuracy',\n",
        "                'Old Test Accuracy',\n",
        "                'Old Val Accuracy'\n",
        "])\n",
        "pFedHN_Gen(global_model, clients, criterion, args, metrics, device, test_dataset) #Ahmad  add your missing argument\n",
        "\"\"\"pFedHN_embedtuning ()\n",
        "      just take the last check pint of the pFedHN run don on 1900 rounds and with 90 clients\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dm32EoKpxmCp",
        "outputId": "ba651681-72ff-4b6a-a1b5-be759b206622"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint data loaded successfully:\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    client_id      train        val       test  new_id  \\\n",
              "0           0  {66: 400}  {66: 100}  {66: 100}      52   \n",
              "1           1  {52: 400}  {52: 100}  {52: 100}      57   \n",
              "2           2  {32: 400}  {32: 100}  {32: 100}      33   \n",
              "3           3  {76: 400}  {76: 100}  {76: 100}       3   \n",
              "4           4  {80: 400}  {80: 100}  {80: 100}      15   \n",
              "..        ...        ...        ...        ...     ...   \n",
              "95         95  {54: 400}  {54: 100}  {54: 100}      37   \n",
              "96         96  {51: 400}  {51: 100}  {51: 100}      32   \n",
              "97         97  {92: 400}  {92: 100}  {92: 100}      56   \n",
              "98         98  {63: 400}  {63: 100}  {63: 100}      85   \n",
              "99         99   {0: 400}   {0: 100}   {0: 100}      28   \n",
              "\n",
              "                                        train_indices  \\\n",
              "0   [21798, 9842, 16198, 36577, 32338, 34686, 3768...   \n",
              "1   [38624, 5427, 28266, 41757, 6122, 3113, 9973, ...   \n",
              "2   [23117, 33712, 37542, 31746, 49253, 36127, 552...   \n",
              "3   [35788, 42003, 28492, 39416, 11844, 35000, 498...   \n",
              "4   [19346, 34367, 26146, 41023, 15974, 41326, 383...   \n",
              "..                                                ...   \n",
              "95  [33406, 33249, 40826, 1577, 19205, 29793, 1580...   \n",
              "96  [2882, 40895, 48859, 13023, 35381, 44138, 1830...   \n",
              "97  [8165, 37937, 8080, 27736, 13140, 29705, 11549...   \n",
              "98  [10625, 6989, 20417, 21242, 6953, 42918, 2716,...   \n",
              "99  [16942, 39404, 2500, 33492, 14480, 19265, 9404...   \n",
              "\n",
              "                                          val_indices  \\\n",
              "0   [4812, 13010, 9025, 37095, 39183, 268, 21995, ...   \n",
              "1   [42526, 23261, 13141, 32989, 44141, 41089, 231...   \n",
              "2   [33040, 38597, 30277, 43613, 26454, 40693, 361...   \n",
              "3   [49443, 10426, 11318, 15173, 36490, 35418, 532...   \n",
              "4   [6440, 45794, 28508, 24698, 3026, 1081, 47146,...   \n",
              "..                                                ...   \n",
              "95  [5071, 27780, 1178, 456, 13428, 36758, 26599, ...   \n",
              "96  [16786, 29546, 33842, 39553, 31485, 33216, 982...   \n",
              "97  [25644, 27435, 8485, 11055, 25695, 32326, 4371...   \n",
              "98  [34607, 147, 8330, 2887, 27506, 41920, 41902, ...   \n",
              "99  [41798, 5129, 32159, 2126, 32507, 2264, 35193,...   \n",
              "\n",
              "                                         test_indices  \n",
              "0   [4075, 2619, 6441, 415, 3660, 2546, 6335, 4618...  \n",
              "1   [1803, 7734, 6137, 2283, 7438, 7666, 5265, 114...  \n",
              "2   [3366, 7480, 2465, 9699, 8860, 4954, 689, 6158...  \n",
              "3   [5062, 5355, 5477, 3097, 6744, 1580, 5547, 524...  \n",
              "4   [5089, 1883, 2769, 208, 3051, 9590, 260, 7760,...  \n",
              "..                                                ...  \n",
              "95  [7598, 7997, 1255, 3632, 9430, 8158, 3903, 322...  \n",
              "96  [9847, 7619, 7356, 8788, 5484, 3337, 7425, 350...  \n",
              "97  [4613, 6896, 1504, 2000, 9938, 9254, 6944, 689...  \n",
              "98  [564, 2046, 8403, 7185, 7449, 5055, 6195, 971,...  \n",
              "99  [8211, 4676, 5496, 3775, 9082, 6806, 9221, 466...  \n",
              "\n",
              "[100 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a01fc5f9-38b2-442d-8fed-7e77f34b71df\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>client_id</th>\n",
              "      <th>train</th>\n",
              "      <th>val</th>\n",
              "      <th>test</th>\n",
              "      <th>new_id</th>\n",
              "      <th>train_indices</th>\n",
              "      <th>val_indices</th>\n",
              "      <th>test_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>{66: 400}</td>\n",
              "      <td>{66: 100}</td>\n",
              "      <td>{66: 100}</td>\n",
              "      <td>52</td>\n",
              "      <td>[21798, 9842, 16198, 36577, 32338, 34686, 3768...</td>\n",
              "      <td>[4812, 13010, 9025, 37095, 39183, 268, 21995, ...</td>\n",
              "      <td>[4075, 2619, 6441, 415, 3660, 2546, 6335, 4618...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>{52: 400}</td>\n",
              "      <td>{52: 100}</td>\n",
              "      <td>{52: 100}</td>\n",
              "      <td>57</td>\n",
              "      <td>[38624, 5427, 28266, 41757, 6122, 3113, 9973, ...</td>\n",
              "      <td>[42526, 23261, 13141, 32989, 44141, 41089, 231...</td>\n",
              "      <td>[1803, 7734, 6137, 2283, 7438, 7666, 5265, 114...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>{32: 400}</td>\n",
              "      <td>{32: 100}</td>\n",
              "      <td>{32: 100}</td>\n",
              "      <td>33</td>\n",
              "      <td>[23117, 33712, 37542, 31746, 49253, 36127, 552...</td>\n",
              "      <td>[33040, 38597, 30277, 43613, 26454, 40693, 361...</td>\n",
              "      <td>[3366, 7480, 2465, 9699, 8860, 4954, 689, 6158...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>{76: 400}</td>\n",
              "      <td>{76: 100}</td>\n",
              "      <td>{76: 100}</td>\n",
              "      <td>3</td>\n",
              "      <td>[35788, 42003, 28492, 39416, 11844, 35000, 498...</td>\n",
              "      <td>[49443, 10426, 11318, 15173, 36490, 35418, 532...</td>\n",
              "      <td>[5062, 5355, 5477, 3097, 6744, 1580, 5547, 524...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>{80: 400}</td>\n",
              "      <td>{80: 100}</td>\n",
              "      <td>{80: 100}</td>\n",
              "      <td>15</td>\n",
              "      <td>[19346, 34367, 26146, 41023, 15974, 41326, 383...</td>\n",
              "      <td>[6440, 45794, 28508, 24698, 3026, 1081, 47146,...</td>\n",
              "      <td>[5089, 1883, 2769, 208, 3051, 9590, 260, 7760,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>{54: 400}</td>\n",
              "      <td>{54: 100}</td>\n",
              "      <td>{54: 100}</td>\n",
              "      <td>37</td>\n",
              "      <td>[33406, 33249, 40826, 1577, 19205, 29793, 1580...</td>\n",
              "      <td>[5071, 27780, 1178, 456, 13428, 36758, 26599, ...</td>\n",
              "      <td>[7598, 7997, 1255, 3632, 9430, 8158, 3903, 322...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>{51: 400}</td>\n",
              "      <td>{51: 100}</td>\n",
              "      <td>{51: 100}</td>\n",
              "      <td>32</td>\n",
              "      <td>[2882, 40895, 48859, 13023, 35381, 44138, 1830...</td>\n",
              "      <td>[16786, 29546, 33842, 39553, 31485, 33216, 982...</td>\n",
              "      <td>[9847, 7619, 7356, 8788, 5484, 3337, 7425, 350...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>{92: 400}</td>\n",
              "      <td>{92: 100}</td>\n",
              "      <td>{92: 100}</td>\n",
              "      <td>56</td>\n",
              "      <td>[8165, 37937, 8080, 27736, 13140, 29705, 11549...</td>\n",
              "      <td>[25644, 27435, 8485, 11055, 25695, 32326, 4371...</td>\n",
              "      <td>[4613, 6896, 1504, 2000, 9938, 9254, 6944, 689...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>{63: 400}</td>\n",
              "      <td>{63: 100}</td>\n",
              "      <td>{63: 100}</td>\n",
              "      <td>85</td>\n",
              "      <td>[10625, 6989, 20417, 21242, 6953, 42918, 2716,...</td>\n",
              "      <td>[34607, 147, 8330, 2887, 27506, 41920, 41902, ...</td>\n",
              "      <td>[564, 2046, 8403, 7185, 7449, 5055, 6195, 971,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>{0: 400}</td>\n",
              "      <td>{0: 100}</td>\n",
              "      <td>{0: 100}</td>\n",
              "      <td>28</td>\n",
              "      <td>[16942, 39404, 2500, 33492, 14480, 19265, 9404...</td>\n",
              "      <td>[41798, 5129, 32159, 2126, 32507, 2264, 35193,...</td>\n",
              "      <td>[8211, 4676, 5496, 3775, 9082, 6806, 9221, 466...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a01fc5f9-38b2-442d-8fed-7e77f34b71df')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a01fc5f9-38b2-442d-8fed-7e77f34b71df button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a01fc5f9-38b2-442d-8fed-7e77f34b71df');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9b2b81f3-86b2-4914-8600-0e670532192c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b2b81f3-86b2-4914-8600-0e670532192c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9b2b81f3-86b2-4914-8600-0e670532192c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6cfca0c5-3290-4968-bf0f-7ce3a44dbf01\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('clients_classes_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6cfca0c5-3290-4968-bf0f-7ce3a44dbf01 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('clients_classes_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "clients_classes_df",
              "summary": "{\n  \"name\": \"clients_classes_df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"client_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          62,\n          13,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    client_id      train        val       test  new_id  \\\n",
              "0          11  {61: 400}  {61: 100}  {61: 100}       0   \n",
              "1          81   {4: 400}   {4: 100}   {4: 100}       1   \n",
              "2          77   {2: 400}   {2: 100}   {2: 100}       2   \n",
              "3           3  {76: 400}  {76: 100}  {76: 100}       3   \n",
              "4          65  {53: 400}  {53: 100}  {53: 100}       4   \n",
              "..        ...        ...        ...        ...     ...   \n",
              "95         60  {29: 400}  {29: 100}  {29: 100}      95   \n",
              "96         12  {98: 400}  {98: 100}  {98: 100}      96   \n",
              "97         50   {7: 400}   {7: 100}   {7: 100}      97   \n",
              "98         71  {12: 400}  {12: 100}  {12: 100}      98   \n",
              "99         58   {6: 400}   {6: 100}   {6: 100}      99   \n",
              "\n",
              "                                        train_indices  \\\n",
              "0   [43031, 42082, 30244, 3309, 18975, 38580, 4852...   \n",
              "1   [30665, 38204, 27342, 39748, 35882, 19181, 135...   \n",
              "2   [9964, 580, 382, 16332, 27661, 12363, 47261, 3...   \n",
              "3   [35788, 42003, 28492, 39416, 11844, 35000, 498...   \n",
              "4   [49422, 26610, 18645, 28859, 44041, 20464, 302...   \n",
              "..                                                ...   \n",
              "95  [31711, 30207, 14744, 31805, 24054, 27887, 158...   \n",
              "96  [45304, 740, 29937, 42097, 47458, 5459, 46462,...   \n",
              "97  [31534, 25681, 5516, 38429, 3810, 7173, 30265,...   \n",
              "98  [38619, 23655, 14684, 19663, 25921, 6031, 4694...   \n",
              "99  [23387, 2017, 35141, 31356, 22066, 1320, 22877...   \n",
              "\n",
              "                                          val_indices  \\\n",
              "0   [38599, 25239, 37441, 29374, 49725, 20029, 492...   \n",
              "1   [46937, 31311, 32117, 954, 10099, 10243, 21109...   \n",
              "2   [46974, 4031, 31300, 13524, 47390, 36345, 8681...   \n",
              "3   [49443, 10426, 11318, 15173, 36490, 35418, 532...   \n",
              "4   [11550, 22135, 49432, 8198, 46248, 13245, 1948...   \n",
              "..                                                ...   \n",
              "95  [36208, 9684, 28788, 29096, 37652, 10355, 3919...   \n",
              "96  [10317, 22963, 11504, 20033, 25848, 34236, 388...   \n",
              "97  [39476, 43315, 23862, 26164, 32032, 14553, 214...   \n",
              "98  [10034, 48415, 6835, 20126, 17653, 35516, 2545...   \n",
              "99  [42085, 8221, 2518, 11745, 33819, 27682, 34185...   \n",
              "\n",
              "                                         test_indices  \n",
              "0   [4562, 9064, 5318, 1492, 8195, 7663, 1625, 197...  \n",
              "1   [8400, 4951, 2720, 4749, 3380, 1379, 9158, 885...  \n",
              "2   [2777, 3397, 2804, 4071, 6301, 4056, 573, 1047...  \n",
              "3   [5062, 5355, 5477, 3097, 6744, 1580, 5547, 524...  \n",
              "4   [1457, 9449, 245, 793, 9956, 2467, 1706, 8903,...  \n",
              "..                                                ...  \n",
              "95  [5598, 4632, 8888, 3459, 8984, 9772, 386, 2050...  \n",
              "96  [3879, 2020, 1468, 5061, 2498, 7114, 8601, 892...  \n",
              "97  [2201, 64, 7684, 8294, 9652, 5518, 3103, 198, ...  \n",
              "98  [6905, 5423, 324, 5814, 7116, 1194, 6861, 1649...  \n",
              "99  [7273, 7839, 535, 2461, 816, 2517, 3995, 7091,...  \n",
              "\n",
              "[100 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-486e8a8e-aa3d-41e0-8bfa-01201de2a78a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>client_id</th>\n",
              "      <th>train</th>\n",
              "      <th>val</th>\n",
              "      <th>test</th>\n",
              "      <th>new_id</th>\n",
              "      <th>train_indices</th>\n",
              "      <th>val_indices</th>\n",
              "      <th>test_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>{61: 400}</td>\n",
              "      <td>{61: 100}</td>\n",
              "      <td>{61: 100}</td>\n",
              "      <td>0</td>\n",
              "      <td>[43031, 42082, 30244, 3309, 18975, 38580, 4852...</td>\n",
              "      <td>[38599, 25239, 37441, 29374, 49725, 20029, 492...</td>\n",
              "      <td>[4562, 9064, 5318, 1492, 8195, 7663, 1625, 197...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>81</td>\n",
              "      <td>{4: 400}</td>\n",
              "      <td>{4: 100}</td>\n",
              "      <td>{4: 100}</td>\n",
              "      <td>1</td>\n",
              "      <td>[30665, 38204, 27342, 39748, 35882, 19181, 135...</td>\n",
              "      <td>[46937, 31311, 32117, 954, 10099, 10243, 21109...</td>\n",
              "      <td>[8400, 4951, 2720, 4749, 3380, 1379, 9158, 885...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>77</td>\n",
              "      <td>{2: 400}</td>\n",
              "      <td>{2: 100}</td>\n",
              "      <td>{2: 100}</td>\n",
              "      <td>2</td>\n",
              "      <td>[9964, 580, 382, 16332, 27661, 12363, 47261, 3...</td>\n",
              "      <td>[46974, 4031, 31300, 13524, 47390, 36345, 8681...</td>\n",
              "      <td>[2777, 3397, 2804, 4071, 6301, 4056, 573, 1047...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>{76: 400}</td>\n",
              "      <td>{76: 100}</td>\n",
              "      <td>{76: 100}</td>\n",
              "      <td>3</td>\n",
              "      <td>[35788, 42003, 28492, 39416, 11844, 35000, 498...</td>\n",
              "      <td>[49443, 10426, 11318, 15173, 36490, 35418, 532...</td>\n",
              "      <td>[5062, 5355, 5477, 3097, 6744, 1580, 5547, 524...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>65</td>\n",
              "      <td>{53: 400}</td>\n",
              "      <td>{53: 100}</td>\n",
              "      <td>{53: 100}</td>\n",
              "      <td>4</td>\n",
              "      <td>[49422, 26610, 18645, 28859, 44041, 20464, 302...</td>\n",
              "      <td>[11550, 22135, 49432, 8198, 46248, 13245, 1948...</td>\n",
              "      <td>[1457, 9449, 245, 793, 9956, 2467, 1706, 8903,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>60</td>\n",
              "      <td>{29: 400}</td>\n",
              "      <td>{29: 100}</td>\n",
              "      <td>{29: 100}</td>\n",
              "      <td>95</td>\n",
              "      <td>[31711, 30207, 14744, 31805, 24054, 27887, 158...</td>\n",
              "      <td>[36208, 9684, 28788, 29096, 37652, 10355, 3919...</td>\n",
              "      <td>[5598, 4632, 8888, 3459, 8984, 9772, 386, 2050...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>12</td>\n",
              "      <td>{98: 400}</td>\n",
              "      <td>{98: 100}</td>\n",
              "      <td>{98: 100}</td>\n",
              "      <td>96</td>\n",
              "      <td>[45304, 740, 29937, 42097, 47458, 5459, 46462,...</td>\n",
              "      <td>[10317, 22963, 11504, 20033, 25848, 34236, 388...</td>\n",
              "      <td>[3879, 2020, 1468, 5061, 2498, 7114, 8601, 892...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>50</td>\n",
              "      <td>{7: 400}</td>\n",
              "      <td>{7: 100}</td>\n",
              "      <td>{7: 100}</td>\n",
              "      <td>97</td>\n",
              "      <td>[31534, 25681, 5516, 38429, 3810, 7173, 30265,...</td>\n",
              "      <td>[39476, 43315, 23862, 26164, 32032, 14553, 214...</td>\n",
              "      <td>[2201, 64, 7684, 8294, 9652, 5518, 3103, 198, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>71</td>\n",
              "      <td>{12: 400}</td>\n",
              "      <td>{12: 100}</td>\n",
              "      <td>{12: 100}</td>\n",
              "      <td>98</td>\n",
              "      <td>[38619, 23655, 14684, 19663, 25921, 6031, 4694...</td>\n",
              "      <td>[10034, 48415, 6835, 20126, 17653, 35516, 2545...</td>\n",
              "      <td>[6905, 5423, 324, 5814, 7116, 1194, 6861, 1649...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>58</td>\n",
              "      <td>{6: 400}</td>\n",
              "      <td>{6: 100}</td>\n",
              "      <td>{6: 100}</td>\n",
              "      <td>99</td>\n",
              "      <td>[23387, 2017, 35141, 31356, 22066, 1320, 22877...</td>\n",
              "      <td>[42085, 8221, 2518, 11745, 33819, 27682, 34185...</td>\n",
              "      <td>[7273, 7839, 535, 2461, 816, 2517, 3995, 7091,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-486e8a8e-aa3d-41e0-8bfa-01201de2a78a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-486e8a8e-aa3d-41e0-8bfa-01201de2a78a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-486e8a8e-aa3d-41e0-8bfa-01201de2a78a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-13e840a5-5c3a-4f9a-8559-97e94e5c7daf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13e840a5-5c3a-4f9a-8559-97e94e5c7daf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-13e840a5-5c3a-4f9a-8559-97e94e5c7daf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3dfebdc2-0b1f-40e0-af55-39b09bb2a0e8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_sorted')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3dfebdc2-0b1f-40e0-af55-39b09bb2a0e8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_sorted');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_sorted",
              "summary": "{\n  \"name\": \"df_sorted\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"client_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          49,\n          30,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"new_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_indices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client ID: 11\n",
            "Train indices: [43031, 42082, 30244, 3309, 18975, 38580, 48521, 34818, 32360, 39908]\n",
            "Val indices: [38599, 25239, 37441, 29374, 49725, 20029, 49205, 37139, 33925, 11216]\n",
            "Test indices: [4562, 9064, 5318, 1492, 8195, 7663, 1625, 1970, 8705, 4814]\n",
            "--------------------------------------------------\n",
            "Client ID: 81\n",
            "Train indices: [30665, 38204, 27342, 39748, 35882, 19181, 13503, 43247, 11798, 2769]\n",
            "Val indices: [46937, 31311, 32117, 954, 10099, 10243, 21109, 14996, 27761, 27678]\n",
            "Test indices: [8400, 4951, 2720, 4749, 3380, 1379, 9158, 8853, 3916, 6109]\n",
            "--------------------------------------------------\n",
            "Client ID: 77\n",
            "Train indices: [9964, 580, 382, 16332, 27661, 12363, 47261, 33073, 31638, 23970]\n",
            "Val indices: [46974, 4031, 31300, 13524, 47390, 36345, 8681, 41812, 22766, 37530]\n",
            "Test indices: [2777, 3397, 2804, 4071, 6301, 4056, 573, 1047, 4257, 7968]\n",
            "--------------------------------------------------\n",
            "Client ID: 3\n",
            "Train indices: [35788, 42003, 28492, 39416, 11844, 35000, 49828, 3527, 30774, 12422]\n",
            "Val indices: [49443, 10426, 11318, 15173, 36490, 35418, 5326, 33764, 45282, 44168]\n",
            "Test indices: [5062, 5355, 5477, 3097, 6744, 1580, 5547, 5248, 9343, 6684]\n",
            "--------------------------------------------------\n",
            "Client ID: 65\n",
            "Train indices: [49422, 26610, 18645, 28859, 44041, 20464, 30240, 3102, 11788, 25515]\n",
            "Val indices: [11550, 22135, 49432, 8198, 46248, 13245, 19484, 36546, 47682, 25167]\n",
            "Test indices: [1457, 9449, 245, 793, 9956, 2467, 1706, 8903, 6516, 7401]\n",
            "--------------------------------------------------\n",
            "/content/drive/MyDrive/MLDL/Cifar-100/Checkpoints/checkpoint_pfedhn_0_1_1_4_epoch_500.pth.tar\n",
            "len: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-dc1137e15ccb>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint '/content/drive/MyDrive/MLDL/Cifar-100/Checkpoints/checkpoint_pfedhn_0_1_1_4_epoch_500.pth.tar' (epoch 500)\n",
            "\n",
            "A saving checkpoint with these parameters exists:\n",
            "Last checkpoint details:\n",
            "Epoch reached: 500\n",
            "User input variables: IID: 0, Participation: uniform, Nc: 1, J: 4\n",
            "Test Accuracy: 55.266666666666666%\n",
            "Test Avg Loss: 4.403396789838751\n",
            "Test Avg Acc: 55.266666666666666%\n",
            "Val Accuracy: 55.57777777777778%\n",
            "Val Avg Loss 4.501711083676956\n",
            "Val Avg Acc 55.57777777777777%\n",
            "\n",
            "\n",
            " the old model accuracy on old client is 0.5526666666666666\n",
            "\n",
            "the new model test accuracy on old client is:  0.5526666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 9/100 [02:18<25:45, 16.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step: 10, New Clients Test Loss: 15.3295,  New Clients Test Acc: 0.0780, Test Loss: 5.5477, Test Acc: 0.4916\n",
            "\n",
            "check acc: 0.5526666666666666\n",
            "\n",
            "new client acc: 0.07799999999999999\n",
            "test_avg_loss_new: 15.3295\n",
            "test_avg_acc_new: 0.078\n",
            "test_loss: 5.5477\n",
            "test_acc: 0.4916\n",
            "val_avg_loss_new: 15.9175\n",
            "val_avg_acc_new: 0.069\n",
            "val_loss: 5.7548\n",
            "val_acc: 0.4908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 19/100 [06:30<28:23, 21.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step: 20, New Clients Test Loss: 9.7928,  New Clients Test Acc: 0.1320, Test Loss: 5.0320, Test Acc: 0.4847\n",
            "\n",
            "check acc: 0.5526666666666666\n",
            "\n",
            "new client acc: 0.132\n",
            "test_avg_loss_new: 9.7928\n",
            "test_avg_acc_new: 0.132\n",
            "test_loss: 5.032\n",
            "test_acc: 0.4847\n",
            "val_avg_loss_new: 10.1745\n",
            "val_avg_acc_new: 0.128\n",
            "val_loss: 5.1478\n",
            "val_acc: 0.4882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 29/100 [11:10<34:48, 29.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step: 30, New Clients Test Loss: 5.3170,  New Clients Test Acc: 0.3480, Test Loss: 4.6924, Test Acc: 0.4926\n",
            "\n",
            "check acc: 0.5526666666666666\n",
            "\n",
            "new client acc: 0.348\n",
            "test_avg_loss_new: 5.317\n",
            "test_avg_acc_new: 0.348\n",
            "test_loss: 4.6924\n",
            "test_acc: 0.4926\n",
            "val_avg_loss_new: 5.4367\n",
            "val_avg_acc_new: 0.367\n",
            "val_loss: 4.7947\n",
            "val_acc: 0.4994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 39/100 [15:58<13:19, 13.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step: 40, New Clients Test Loss: 4.6805,  New Clients Test Acc: 0.4100, Test Loss: 4.7765, Test Acc: 0.4845\n",
            "\n",
            "check acc: 0.5526666666666666\n",
            "\n",
            "new client acc: 0.41\n",
            "test_avg_loss_new: 4.6805\n",
            "test_avg_acc_new: 0.41\n",
            "test_loss: 4.7765\n",
            "test_acc: 0.4845\n",
            "val_avg_loss_new: 4.7664\n",
            "val_avg_acc_new: 0.4\n",
            "val_loss: 4.7633\n",
            "val_acc: 0.4931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▉     | 49/100 [19:31<10:41, 12.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step: 50, New Clients Test Loss: 2.7460,  New Clients Test Acc: 0.5490, Test Loss: 4.7114, Test Acc: 0.4854\n",
            "\n",
            "check acc: 0.5526666666666666\n",
            "\n",
            "new client acc: 0.549\n",
            "test_avg_loss_new: 2.746\n",
            "test_avg_acc_new: 0.549\n",
            "test_loss: 4.7114\n",
            "test_acc: 0.4854\n",
            "val_avg_loss_new: 2.7009\n",
            "val_avg_acc_new: 0.56\n",
            "val_loss: 4.7927\n",
            "val_acc: 0.4969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 59/100 [23:02<10:23, 15.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step: 60, New Clients Test Loss: 1.9918,  New Clients Test Acc: 0.6400, Test Loss: 4.7545, Test Acc: 0.4841\n",
            "\n",
            "check acc: 0.5526666666666666\n",
            "\n",
            "new client acc: 0.64\n",
            "test_avg_loss_new: 1.9918\n",
            "test_avg_acc_new: 0.64\n",
            "test_loss: 4.7545\n",
            "test_acc: 0.4841\n",
            "val_avg_loss_new: 1.8842\n",
            "val_avg_acc_new: 0.66\n",
            "val_loss: 4.7816\n",
            "val_acc: 0.498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 59/100 [24:05<16:44, 24.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence found\n",
            "Global Test Acc Diff: 0.06856666666666666\n",
            "Global Val Acc Diff: 0.05777777777777776\n",
            "New Clients Test Acc: 0.64\n",
            "New Clients Val Acc: 0.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pFedHN_embedtuning ()\\n      just take the last check pint of the pFedHN run don on 1900 rounds and with 90 clients'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from tqdm import trange\n",
        "import pickle\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "args = args_parser()\n",
        "args.iid = 0\n",
        "args.participation = 1\n",
        "args.algorithm = \"pfedhn\"\n",
        "args.Nc = 1\n",
        "args.local_ep = 4\n",
        "args.checkpoint_resume = 1\n",
        "args.checkpoint_path = \"/content/drive/MyDrive/MLDL/Cifar-100/Checkpoints\"\n",
        "\n",
        "if args.gpu:\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "device = 'cuda' if args.gpu else 'cpu'\n",
        "\n",
        "train_set, test_set, clients = get_dataset(args)\n",
        "\n",
        "def get_class_distribution(indices, dataset):\n",
        "            targets = [dataset.targets[idx] for idx in indices]\n",
        "            return dict(Counter(targets))\n",
        "\n",
        "for client in clients:\n",
        "\n",
        "    train_dist = get_class_distribution(client.train_indices, client.train_dataset)\n",
        "    val_dist = get_class_distribution(client.val_indices, client.train_dataset)\n",
        "    test_dist = get_class_distribution(client.test_indices, client.test_dataset)\n",
        "\n",
        "    print(f\"Client {client.client_id} class distribution:\")\n",
        "    print(f\"  Train: {train_dist}\")\n",
        "    print(f\"  Val: {val_dist}\")\n",
        "    print(f\"  Test: {test_dist}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "5gZ8En2zxlkm",
        "outputId": "4a1682e5-ce99-4a14-aaed-3f195a8718b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-33cc4ffa1f19>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_class_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ba024cadf6bc>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5071\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4867\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4408\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.2675\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2565\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2761\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         ])\n\u001b[0;32m---> 20\u001b[0;31m         train_dataset = datasets.CIFAR100(data_dir, train=True, download=True,\n\u001b[0m\u001b[1;32m     21\u001b[0m                                        transform=transform_train)\n\u001b[1;32m     22\u001b[0m         test_dataset = datasets.CIFAR100(data_dir, train=False, download=True,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files already downloaded and verified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m_check_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcheck_integrity\u001b[0;34m(fpath, md5)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcheck_md5\u001b[0;34m(fpath, md5, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcalculate_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcalculate_md5\u001b[0;34m(fpath, chunk_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmd5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mmd5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}